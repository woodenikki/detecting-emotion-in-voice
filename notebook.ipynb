{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93a2a579",
   "metadata": {},
   "source": [
    "# MoodWave: Voice-Driven Emotion Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5805244",
   "metadata": {},
   "source": [
    "## Data Guidelines\n",
    "\n",
    "Your dataset must be:\n",
    "\n",
    "- Appropriate for classification. It should have a categorical outcome or the data needed to engineer one.\n",
    "\n",
    "- Usable to solve a specific business problem. This solution must rely on your classification model.\n",
    "\n",
    "- Somewhat complex. It should contain a minimum of 1000 rows and 10 features.\n",
    "\n",
    "- Unfamiliar. It can't be one we've already worked with during the course or that is commonly used for demonstration purposes (e.g. Titanic).\n",
    "\n",
    "- Manageable. Stick to datasets that you can model using the techniques introduced in Phase 3.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6534a806",
   "metadata": {},
   "source": [
    "### Phase 3 Concepts used in this project:\n",
    "\n",
    "- Logistic Regression:\n",
    "\n",
    "> Logistic regression is a fundamental classification algorithm that's well-suited for binary and multiclass classification tasks. It's a good choice if your dataset has clear decision boundaries.\n",
    "\n",
    "- Decision Trees:\n",
    "\n",
    "> Decision trees are versatile and interpretable models that can handle both categorical and continuous data. They are particularly useful when you want to understand the decision-making process of your model.\n",
    "\n",
    "- Evaluation Metrics (Confusion Matrices, ROC Curves, AUC):\n",
    "\n",
    "> These metrics are essential for assessing the performance of your classification model. They will help you understand how well your model distinguishes between different emotional states.\n",
    "\n",
    "- Hyperparameter Tuning and Pruning:\n",
    "\n",
    "> When using decision trees, tuning hyperparameters and pruning are important to avoid overfitting and to ensure your model generalizes well to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6551814",
   "metadata": {},
   "source": [
    "## Data Preperation\n",
    "\n",
    "Here 4 most popular datasets in English: Crema, Ravdess, Savee and Tess. Each of them contains audio in .wav format with some main labels.\n",
    "\n",
    "Because our data isn't inherinantly in a csv / dataframe format, we will have to create it from scratch!\n",
    "\n",
    "First, we will pull all data into their own dataframe, making note of *where* the file is, so we can pull our features from each audio file:\n",
    "\n",
    "- Mel-frequency cepstral coefficients (MFCCs)\n",
    "- Spectral centroid\n",
    "- Chroma features\n",
    "- Zero-crossing rate\n",
    "- RMS energy\n",
    "- Pitch\n",
    "\n",
    "And then of course, our target feature: **Emotion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "597dace5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import zipfile\n",
    "import librosa\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "575f6441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data is zipped, and stored in folders for which dataset they came from:\n",
    "\n",
    "# Define the path to the zipped dataset\n",
    "zip_file_path = 'dataset.zip'\n",
    "extracted_folder_path = 'dataset'\n",
    "\n",
    "# Unzip the dataset\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extracted_folder_path)\n",
    "\n",
    "# Crema\n",
    "# Ravdess\n",
    "# Savee\n",
    "# Tess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d8067e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              filename  emotion           path\n",
      "0  1001_DFA_ANG_XX.wav    angry  dataset\\Crema\n",
      "1  1001_DFA_DIS_XX.wav  disgust  dataset\\Crema\n",
      "2  1001_DFA_FEA_XX.wav     fear  dataset\\Crema\n",
      "3  1001_DFA_HAP_XX.wav    happy  dataset\\Crema\n",
      "4  1001_DFA_NEU_XX.wav  neutral  dataset\\Crema\n"
     ]
    }
   ],
   "source": [
    "# Define the path to the Crema folder\n",
    "crema_folder_path = os.path.join(extracted_folder_path, 'Crema')\n",
    "\n",
    "# Verify that we can access the files and extract emotion labels\n",
    "data = []\n",
    "\n",
    "# Loop through each file in the Crema folder\n",
    "for file_name in os.listdir(crema_folder_path):\n",
    "    if file_name.endswith('.wav'):\n",
    "        # Extract the emotion label from the filename\n",
    "        parts = file_name.split('_')\n",
    "        emotion_code = parts[2]\n",
    "        \n",
    "        # Map the emotion code to the actual emotion label\n",
    "        emotion_map = {\n",
    "            'SAD': 'sadness',\n",
    "            'ANG': 'angry',\n",
    "            'DIS': 'disgust',\n",
    "            'FEA': 'fear',\n",
    "            'HAP': 'happy',\n",
    "            'NEU': 'neutral'\n",
    "        }\n",
    "        emotion_label = emotion_map.get(emotion_code, 'unknown')\n",
    "        \n",
    "        # Store the data with the directory path minus the filename\n",
    "        data.append({'filename': file_name, 'emotion': emotion_label, 'path': crema_folder_path})\n",
    "\n",
    "# Convert the data into a DataFrame for easy access\n",
    "df_crema = pd.DataFrame(data)\n",
    "\n",
    "# Display the first few rows to verify\n",
    "print(df_crema.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb33541b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             filename emotion                    path\n",
      "0  OAF_back_angry.wav   angry  dataset\\Tess\\OAF_angry\n",
      "1   OAF_bar_angry.wav   angry  dataset\\Tess\\OAF_angry\n",
      "2  OAF_base_angry.wav   angry  dataset\\Tess\\OAF_angry\n",
      "3  OAF_bath_angry.wav   angry  dataset\\Tess\\OAF_angry\n",
      "4  OAF_bean_angry.wav   angry  dataset\\Tess\\OAF_angry\n"
     ]
    }
   ],
   "source": [
    "# Define the path to the Tess folder\n",
    "tess_folder_path = os.path.join(extracted_folder_path, 'Tess')\n",
    "\n",
    "# Prepare to store the data\n",
    "data = []\n",
    "\n",
    "# Loop through each emotion folder in the Tess directory\n",
    "for emotion_folder in os.listdir(tess_folder_path):\n",
    "    # Get the full path to the emotion folder\n",
    "    emotion_folder_path = os.path.join(tess_folder_path, emotion_folder)\n",
    "    \n",
    "    # Extract the emotion from the folder name (e.g., \"OAF_angry\" -> \"angry\")\n",
    "    emotion_label = emotion_folder.split('_')[1]\n",
    "    \n",
    "    # Loop through each file in the emotion folder\n",
    "    for file_name in os.listdir(emotion_folder_path):\n",
    "        if file_name.endswith('.wav'):\n",
    "            # Store the data with the directory path minus the filename\n",
    "            data.append({\n",
    "                'filename': file_name, \n",
    "                'emotion': emotion_label, \n",
    "                'path': emotion_folder_path\n",
    "            })\n",
    "\n",
    "# Convert the data into a DataFrame for easy access\n",
    "df_tess = pd.DataFrame(data)\n",
    "\n",
    "# Display the first few rows to verify\n",
    "print(df_tess.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed5da22f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     filename  emotion           path\n",
      "0  DC_a01.wav  unknown  dataset\\Savee\n",
      "1  DC_a02.wav  unknown  dataset\\Savee\n",
      "2  DC_a03.wav  unknown  dataset\\Savee\n",
      "3  DC_a04.wav  unknown  dataset\\Savee\n",
      "4  DC_a05.wav  unknown  dataset\\Savee\n"
     ]
    }
   ],
   "source": [
    "# Define the path to the Savee folder\n",
    "savee_folder_path = os.path.join(extracted_folder_path, 'Savee')\n",
    "\n",
    "# Prepare to store the data\n",
    "data = []\n",
    "\n",
    "# Define the emotion mapping based on the prefixes\n",
    "emotion_map = {\n",
    "    'a': 'anger',\n",
    "    'd': 'disgust',\n",
    "    'f': 'fear',\n",
    "    'h': 'happiness',\n",
    "    'n': 'neutral',\n",
    "    'sa': 'sadness',\n",
    "    'su': 'surprise'\n",
    "}\n",
    "\n",
    "# Loop through each file in the Savee folder\n",
    "for file_name in os.listdir(savee_folder_path):\n",
    "    if file_name.endswith('.wav'):\n",
    "        # Extract the prefix from the filename to determine the emotion\n",
    "        prefix = file_name.split('_')[1][:2]\n",
    "        \n",
    "        # Map the prefix to the corresponding emotion\n",
    "        emotion_label = emotion_map.get(prefix, 'unknown')\n",
    "        \n",
    "        # Store the data with the directory path minus the filename\n",
    "        data.append({\n",
    "            'filename': file_name, \n",
    "            'emotion': emotion_label, \n",
    "            'path': savee_folder_path\n",
    "        })\n",
    "\n",
    "# Convert the data into a DataFrame for easy access\n",
    "df_savee = pd.DataFrame(data)\n",
    "\n",
    "# Display the first few rows to verify\n",
    "print(df_savee.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7910385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   filename  emotion  \\\n",
      "0  03-01-01-01-01-01-01.wav  neutral   \n",
      "1  03-01-01-01-01-02-01.wav  neutral   \n",
      "2  03-01-01-01-02-01-01.wav  neutral   \n",
      "3  03-01-01-01-02-02-01.wav  neutral   \n",
      "4  03-01-02-01-01-01-01.wav     calm   \n",
      "\n",
      "                                                path  \n",
      "0  dataset\\Ravdess\\audio_speech_actors_01-24\\Acto...  \n",
      "1  dataset\\Ravdess\\audio_speech_actors_01-24\\Acto...  \n",
      "2  dataset\\Ravdess\\audio_speech_actors_01-24\\Acto...  \n",
      "3  dataset\\Ravdess\\audio_speech_actors_01-24\\Acto...  \n",
      "4  dataset\\Ravdess\\audio_speech_actors_01-24\\Acto...  \n"
     ]
    }
   ],
   "source": [
    "# Define the path to the Ravdess folder\n",
    "ravdess_folder_path = os.path.join(extracted_folder_path, 'Ravdess', 'audio_speech_actors_01-24')\n",
    "\n",
    "# Prepare to store the data\n",
    "data = []\n",
    "\n",
    "# Define the emotion mapping based on the third component in the filename\n",
    "emotion_map = {\n",
    "    '01': 'neutral',\n",
    "    '02': 'calm',\n",
    "    '03': 'happy',\n",
    "    '04': 'sad',\n",
    "    '05': 'angry',\n",
    "    '06': 'fearful',\n",
    "    '07': 'disgust',\n",
    "    '08': 'surprised'\n",
    "}\n",
    "\n",
    "# Loop through each actor's folder in the Ravdess directory\n",
    "for actor_folder in os.listdir(ravdess_folder_path):\n",
    "    actor_folder_path = os.path.join(ravdess_folder_path, actor_folder)\n",
    "    \n",
    "    # Loop through each file in the actor's folder\n",
    "    for file_name in os.listdir(actor_folder_path):\n",
    "        if file_name.endswith('.wav'):\n",
    "            # Extract the third component from the filename to determine the emotion\n",
    "            emotion_code = file_name.split('-')[2]\n",
    "            \n",
    "            # Map the emotion code to the corresponding emotion label\n",
    "            emotion_label = emotion_map.get(emotion_code, 'unknown')\n",
    "            \n",
    "            # Store the data with the directory path minus the filename\n",
    "            data.append({\n",
    "                'filename': file_name, \n",
    "                'emotion': emotion_label, \n",
    "                'path': actor_folder_path\n",
    "            })\n",
    "\n",
    "# Convert the data into a DataFrame for easy access\n",
    "df_ravdess = pd.DataFrame(data)\n",
    "\n",
    "# Display the first few rows to verify\n",
    "print(df_ravdess.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5e8750",
   "metadata": {},
   "source": [
    "### combining datasets \n",
    "\n",
    "We will merge the datsets into one dataframe, and assign unique identifiers\n",
    "- Concatenate the DataFrames for each dataset.\n",
    "- Assign a unique ID to each entry based on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6558a220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       id             filename  emotion           path\n",
      "0  c_0001  1001_DFA_ANG_XX.wav    angry  dataset\\Crema\n",
      "1  c_0002  1001_DFA_DIS_XX.wav  disgust  dataset\\Crema\n",
      "2  c_0003  1001_DFA_FEA_XX.wav     fear  dataset\\Crema\n",
      "3  c_0004  1001_DFA_HAP_XX.wav    happy  dataset\\Crema\n",
      "4  c_0005  1001_DFA_NEU_XX.wav  neutral  dataset\\Crema\n"
     ]
    }
   ],
   "source": [
    "# Add a unique ID column to each dataset\n",
    "df_crema['id'] = ['c_{:04d}'.format(i + 1) for i in range(len(df_crema))]\n",
    "df_tess['id'] = ['t_{:04d}'.format(i + 1) for i in range(len(df_tess))]\n",
    "df_savee['id'] = ['s_{:04d}'.format(i + 1) for i in range(len(df_savee))]\n",
    "df_ravdess['id'] = ['r_{:04d}'.format(i + 1) for i in range(len(df_ravdess))]\n",
    "\n",
    "# Merge the datasets into a single DataFrame\n",
    "merged_data = pd.concat([df_crema, df_tess, df_savee, df_ravdess], ignore_index=True)\n",
    "\n",
    "# Reorder columns to have 'id' as the first column\n",
    "merged_data = merged_data[['id', 'filename', 'emotion', 'path']]\n",
    "\n",
    "# Display the first few rows of the combined DataFrame\n",
    "print(merged_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "179694c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows in dataset: 12162\n"
     ]
    }
   ],
   "source": [
    "# remember, we need at least 1000 rows to meet our requirements. \n",
    "print(f\"Total rows in dataset: {merged_data.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17da664f",
   "metadata": {},
   "source": [
    "### Extracting Features\n",
    "\n",
    "Again, these are the features we will extract:\n",
    "\n",
    "- **Mel-frequency cepstral coefficients (MFCCs):** Represents the short-term power spectrum of sound, commonly used in speech and audio processing to capture the timbral texture of audio.\n",
    "- **Spectral centroid:** Indicates the \"center of mass\" of the spectrum and is often associated with the perceived brightness of a sound.\n",
    "- **Chroma features:** Represents the 12 different pitch classes and captures harmonic and melodic characteristics of music / voice.\n",
    "- **Zero-crossing rate:** Measures the rate at which the signal changes sign, giving insight into the noisiness or percussiveness of the sound.\n",
    "- **RMS energy:** Reflects the root mean square of the audio signal and indicates the energy or loudness of the sound.\n",
    "- **Pitch:** Refers to the perceived frequency of a sound, determining how high or low a sound is.\n",
    "\n",
    "We will be using the `librosa` package to process these audio features. [Here](https://librosa.org/doc/latest/index.html) is a link to the librosa documentation.\n",
    "\n",
    "**Note**: *adding suppression for UserWarning: Trying to estimate tuning from empty frequency set. This is likely do to either:* **silence / low energy** *(too quiet to perform reliable pitch estimation), or the file had* **too short of a duration**. *This warning shows up even when setting the pitch to 0 in this case.*\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "33599208",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(file_path):\n",
    "    y, sr = librosa.load(file_path, sr=None)\n",
    "    \n",
    "    # Extract MFCCs\n",
    "    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "    mfccs_mean = np.mean(mfccs, axis=1)\n",
    "\n",
    "    # Extract Spectral Centroid\n",
    "    spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
    "    spectral_centroid_mean = np.mean(spectral_centroid)\n",
    "\n",
    "    # Extract Chroma Features\n",
    "    chroma_stft = librosa.feature.chroma_stft(y=y, sr=sr)\n",
    "    chroma_stft_mean = np.mean(chroma_stft, axis=1)\n",
    "\n",
    "    # Extract Zero-Crossing Rate\n",
    "    zero_crossing_rate = librosa.feature.zero_crossing_rate(y)\n",
    "    zero_crossing_rate_mean = np.mean(zero_crossing_rate)\n",
    "\n",
    "    # Extract RMS Energy\n",
    "    rms = librosa.feature.rms(y=y)\n",
    "    rms_mean = np.mean(rms)\n",
    "\n",
    "    # Extract Pitch\n",
    "    pitches, magnitudes = librosa.core.piptrack(y=y, sr=sr)\n",
    "    \n",
    "    # Avoid estimating pitch from an empty frequency set\n",
    "    if pitches.size > 0 and np.sum(pitches) > 0:\n",
    "        pitch = np.mean(pitches[pitches > 0])\n",
    "    else:\n",
    "        pitch = 0  # too quiet to \n",
    "\n",
    "    return mfccs_mean, spectral_centroid_mean, chroma_stft_mean, zero_crossing_rate_mean, rms_mean, pitch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "35d5214a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MFCCs: [-306.0274       92.670235      8.491312     23.965403      7.4779935\n",
      "   -5.759455    -11.883088     -9.676736     -3.9967465   -13.352565\n",
      "    0.40819725   -9.709486     -6.1271243 ]\n",
      "Spectral Centroid: 1584.9930703294388\n",
      "Chroma Features: [0.37491405 0.37949282 0.41722107 0.39018238 0.4148401  0.2977837\n",
      " 0.28898865 0.3575554  0.35190624 0.42918485 0.6879576  0.5454907 ]\n",
      "Zero-Crossing Rate: 0.10186767578125\n",
      "RMS Energy: 0.041986194\n",
      "Pitch: 1211.9507\n"
     ]
    }
   ],
   "source": [
    "# testing our extract_features function:\n",
    "\n",
    "first_row = merged_data.iloc[0]\n",
    "file_path = os.path.join(first_row['path'], first_row['filename'])\n",
    "\n",
    "# Extract features\n",
    "features = extract_features(file_path)\n",
    "\n",
    "# Print out each feature with its corresponding values\n",
    "print(\"MFCCs:\", features[0])\n",
    "print(\"Spectral Centroid:\", features[1])\n",
    "print(\"Chroma Features:\", features[2])\n",
    "print(\"Zero-Crossing Rate:\", features[3])\n",
    "print(\"RMS Energy:\", features[4])\n",
    "print(\"Pitch:\", features[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c436aa8c",
   "metadata": {},
   "source": [
    "### Validating the Values:\n",
    "\n",
    "- **MFCCs:** Typically, MFCC values range from -400 to 400, depending on the scale of the input signal.\n",
    "> All values: **pass**\n",
    "\n",
    "- **Spectral Centroid:** This value represents the \"center of mass\" of the spectrum and typically ranges between 0 and the - Nyquist frequency (half the sampling rate).\n",
    "> 1584.99: **pass**\n",
    "\n",
    "- **Chroma Features:** These represent the energy distribution across 12 pitch classes. They are normalized, so values between 0 and 1 are expected.\n",
    "> All values: **pass**\n",
    "\n",
    "- **Zero-Crossing Rate:** This rate indicates how frequently the signal changes sign. It ranges from 0 to 1. \n",
    "> 0.1018: **pass**\n",
    "\n",
    "- **RMS Energy:** This value should be within the range of 0 to 1 for normalized signals.\n",
    "> 0.0419: **pass**\n",
    "\n",
    "- **Pitch:** Pitch values are measured in Hz, and depends on the type of audio.\n",
    "> 1211.95: **pass**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617b92d6",
   "metadata": {},
   "source": [
    "Now that we've validated our extract_features function, we can apply it to the rest of our dataframe.\n",
    "\n",
    "**Notes**: \n",
    "- This cell can take a while to run! About 5 minutes\n",
    "- suppressed UserWarning: Trying to estimate tuning from empty frequency set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8923e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty lists to store features\n",
    "mfccs_list = []\n",
    "spectral_centroid_list = []\n",
    "chroma_list = []\n",
    "zero_crossing_rate_list = []\n",
    "rms_list = []\n",
    "pitch_list = []\n",
    "\n",
    "# Iterate over each row in the DataFrame\n",
    "for index, row in merged_data.iterrows():\n",
    "    file_path = os.path.join(row['path'], row['filename'])\n",
    "    mfccs, spectral_centroid, chroma, zcr, rms, pitch = extract_features(file_path)\n",
    "    \n",
    "    mfccs_list.append(mfccs)\n",
    "    spectral_centroid_list.append(spectral_centroid)\n",
    "    chroma_list.append(chroma)\n",
    "    zero_crossing_rate_list.append(zcr)\n",
    "    rms_list.append(rms)\n",
    "    pitch_list.append(pitch)\n",
    "\n",
    "# Add the features to the DataFrame\n",
    "merged_data['mfccs'] = mfccs_list\n",
    "merged_data['spectral_centroid'] = spectral_centroid_list\n",
    "merged_data['chroma'] = chroma_list\n",
    "merged_data['zero_crossing_rate'] = zero_crossing_rate_list\n",
    "merged_data['rms'] = rms_list\n",
    "merged_data['pitch'] = pitch_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b23dabe2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>filename</th>\n",
       "      <th>emotion</th>\n",
       "      <th>path</th>\n",
       "      <th>mfccs</th>\n",
       "      <th>spectral_centroid</th>\n",
       "      <th>chroma</th>\n",
       "      <th>zero_crossing_rate</th>\n",
       "      <th>rms</th>\n",
       "      <th>pitch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c_0001</td>\n",
       "      <td>1001_DFA_ANG_XX.wav</td>\n",
       "      <td>angry</td>\n",
       "      <td>dataset\\Crema</td>\n",
       "      <td>[-306.0274, 92.670235, 8.491312, 23.965403, 7....</td>\n",
       "      <td>1584.993070</td>\n",
       "      <td>[0.37491405, 0.37949282, 0.41722107, 0.3901823...</td>\n",
       "      <td>0.101868</td>\n",
       "      <td>0.041986</td>\n",
       "      <td>1211.950684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c_0002</td>\n",
       "      <td>1001_DFA_DIS_XX.wav</td>\n",
       "      <td>disgust</td>\n",
       "      <td>dataset\\Crema</td>\n",
       "      <td>[-346.39963, 95.83912, 10.516282, 31.619215, 1...</td>\n",
       "      <td>1531.650487</td>\n",
       "      <td>[0.47289878, 0.4768195, 0.33598945, 0.34610763...</td>\n",
       "      <td>0.093061</td>\n",
       "      <td>0.015996</td>\n",
       "      <td>1256.617188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c_0003</td>\n",
       "      <td>1001_DFA_FEA_XX.wav</td>\n",
       "      <td>fear</td>\n",
       "      <td>dataset\\Crema</td>\n",
       "      <td>[-321.42026, 94.76091, 8.155397, 23.323242, 11...</td>\n",
       "      <td>1489.088839</td>\n",
       "      <td>[0.3272673, 0.39935032, 0.35215598, 0.38248017...</td>\n",
       "      <td>0.084286</td>\n",
       "      <td>0.045776</td>\n",
       "      <td>992.574402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>c_0004</td>\n",
       "      <td>1001_DFA_HAP_XX.wav</td>\n",
       "      <td>happy</td>\n",
       "      <td>dataset\\Crema</td>\n",
       "      <td>[-303.30374, 92.52889, 4.231231, 27.970133, 10...</td>\n",
       "      <td>1555.376035</td>\n",
       "      <td>[0.3150873, 0.31478375, 0.30918238, 0.3423785,...</td>\n",
       "      <td>0.084878</td>\n",
       "      <td>0.042300</td>\n",
       "      <td>1102.953003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c_0005</td>\n",
       "      <td>1001_DFA_NEU_XX.wav</td>\n",
       "      <td>neutral</td>\n",
       "      <td>dataset\\Crema</td>\n",
       "      <td>[-335.4959, 100.39331, 9.384935, 30.160904, 11...</td>\n",
       "      <td>1495.394997</td>\n",
       "      <td>[0.4112704, 0.36269408, 0.3349767, 0.32547352,...</td>\n",
       "      <td>0.082031</td>\n",
       "      <td>0.020450</td>\n",
       "      <td>1041.093628</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id             filename  emotion           path  \\\n",
       "0  c_0001  1001_DFA_ANG_XX.wav    angry  dataset\\Crema   \n",
       "1  c_0002  1001_DFA_DIS_XX.wav  disgust  dataset\\Crema   \n",
       "2  c_0003  1001_DFA_FEA_XX.wav     fear  dataset\\Crema   \n",
       "3  c_0004  1001_DFA_HAP_XX.wav    happy  dataset\\Crema   \n",
       "4  c_0005  1001_DFA_NEU_XX.wav  neutral  dataset\\Crema   \n",
       "\n",
       "                                               mfccs  spectral_centroid  \\\n",
       "0  [-306.0274, 92.670235, 8.491312, 23.965403, 7....        1584.993070   \n",
       "1  [-346.39963, 95.83912, 10.516282, 31.619215, 1...        1531.650487   \n",
       "2  [-321.42026, 94.76091, 8.155397, 23.323242, 11...        1489.088839   \n",
       "3  [-303.30374, 92.52889, 4.231231, 27.970133, 10...        1555.376035   \n",
       "4  [-335.4959, 100.39331, 9.384935, 30.160904, 11...        1495.394997   \n",
       "\n",
       "                                              chroma  zero_crossing_rate  \\\n",
       "0  [0.37491405, 0.37949282, 0.41722107, 0.3901823...            0.101868   \n",
       "1  [0.47289878, 0.4768195, 0.33598945, 0.34610763...            0.093061   \n",
       "2  [0.3272673, 0.39935032, 0.35215598, 0.38248017...            0.084286   \n",
       "3  [0.3150873, 0.31478375, 0.30918238, 0.3423785,...            0.084878   \n",
       "4  [0.4112704, 0.36269408, 0.3349767, 0.32547352,...            0.082031   \n",
       "\n",
       "        rms        pitch  \n",
       "0  0.041986  1211.950684  \n",
       "1  0.015996  1256.617188  \n",
       "2  0.045776   992.574402  \n",
       "3  0.042300  1102.953003  \n",
       "4  0.020450  1041.093628  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c0e602",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
