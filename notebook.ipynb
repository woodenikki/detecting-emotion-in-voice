{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37722741",
   "metadata": {},
   "source": [
    "# MoodWave: Voice-Driven Emotion Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "659b7c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import zipfile\n",
    "import librosa\n",
    "import random\n",
    "import os\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures, LabelEncoder, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import resample\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "from scipy.stats import skew, kurtosis\n",
    "from collections import Counter\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6e88ae",
   "metadata": {},
   "source": [
    "## Data Guidelines\n",
    "\n",
    "Your dataset must be:\n",
    "\n",
    "- Appropriate for classification. It should have a categorical outcome or the data needed to engineer one.\n",
    "\n",
    "- Usable to solve a specific business problem. This solution must rely on your classification model.\n",
    "\n",
    "- Somewhat complex. It should contain a minimum of 1000 rows and 10 features.\n",
    "\n",
    "- Unfamiliar. It can't be one we've already worked with during the course or that is commonly used for demonstration purposes (e.g. Titanic).\n",
    "\n",
    "- Manageable. Stick to datasets that you can model using the techniques introduced in Phase 3.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca496151",
   "metadata": {},
   "source": [
    "## Data Understanding\n",
    "\n",
    "In this project, we are dealing with four datasets containing audio recordings in the .wav format. Each audio recording is labeled with an emotion that the speaker is evoking in their statement. Our goal is to build a model that can successfully map an emotion to a given voice clip of someone speaking.\n",
    "\n",
    "To achieve this, we will extract various features from the audio recordings that are relevant for analyzing speech and emotion. Here are the features we will be working with and their significance:\n",
    "\n",
    "1. **Mel-frequency Cepstral Coefficients (MFCCs):** MFCCs are a compact representation of the short-term power spectrum of a sound. They are widely used in speech recognition and audio analysis tasks because they capture the essential characteristics of the audio signal while being robust to noise and other variabilities. MFCCs are particularly useful for identifying the phonetic content of speech, which can be helpful in determining the emotional state of the speaker.\n",
    "\n",
    "2. **Spectral Centroid:** The spectral centroid is a measure of the brightness or sharpness of a sound. It represents the weighted mean frequency of the spectrum and can be used to distinguish between different types of sounds or emotions. For example, a bright, harsh sound might have a higher spectral centroid than a mellow, soft sound.\n",
    "\n",
    "3. **Chroma Features:** Chroma features describe the distribution of energy across different pitch classes (notes) in the audio signal. They are useful for capturing tonal information, which can be relevant for identifying emotions in speech, particularly those related to intonation patterns and stress.\n",
    "\n",
    "4. **Zero-Crossing Rate:** The zero-crossing rate is a measure of the number of times the audio signal crosses the zero amplitude axis within a given time frame. It can be used to distinguish between different types of sounds, such as voiced and unvoiced speech, and can provide insights into the energy distribution of the audio signal.\n",
    "RMS Energy: The Root Mean Square (RMS) energy is a measure of the overall energy or loudness of an audio signal. It can be useful for detecting variations in volume or intensity, which can be indicative of certain emotions, such as anger or excitement.\n",
    "\n",
    "5. **Pitch:** The pitch feature represents the fundamental frequency of the audio signal. It is closely related to the perception of tone and can be useful for analyzing the intonation patterns and stress levels in speech, which can be indicators of different emotional states.\n",
    "\n",
    "---\n",
    "\n",
    "By extracting and analyzing these features, we can capture various acoustic characteristics of the speech signal that may be relevant for distinguishing between different emotions. This multi-faceted approach can provide a more comprehensive representation of the audio data, potentially leading to better performance in the emotion classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e274c73",
   "metadata": {},
   "source": [
    "## Data Preperation\n",
    "\n",
    "Here 4 most popular datasets in English: Crema, Ravdess, Savee and Tess. Each of them contains audio in .wav format with some main labels.\n",
    "\n",
    "Because our data isn't inherinantly in a csv / dataframe format, we will have to create it from scratch!\n",
    "\n",
    "First, we will pull all data into their own dataframe, making note of *where* the file is, so we can later pull our features from each audio file. \n",
    "\n",
    "After each dataset has been imported into its own dataframe, we will merge them all into one dataframe. Then, we can extract our desired audio features:\n",
    "\n",
    "- Mel-frequency cepstral coefficients (MFCCs)\n",
    "- Spectral centroid\n",
    "- Chroma features\n",
    "- Zero-crossing rate\n",
    "- RMS energy\n",
    "- Pitch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f76d2ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data is zipped, and stored in folders for which dataset they came from:\n",
    "\n",
    "# Define the path to the zipped dataset\n",
    "zip_file_path = 'dataset.zip'\n",
    "extracted_folder_path = 'dataset'\n",
    "\n",
    "# Unzip the dataset\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extracted_folder_path)\n",
    "\n",
    "# Crema\n",
    "# Ravdess\n",
    "# Savee\n",
    "# Tess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fdd31145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              filename  emotion           path\n",
      "0  1001_DFA_ANG_XX.wav    angry  dataset\\Crema\n",
      "1  1001_DFA_DIS_XX.wav  disgust  dataset\\Crema\n",
      "2  1001_DFA_FEA_XX.wav     fear  dataset\\Crema\n",
      "3  1001_DFA_HAP_XX.wav    happy  dataset\\Crema\n",
      "4  1001_DFA_NEU_XX.wav  neutral  dataset\\Crema\n"
     ]
    }
   ],
   "source": [
    "# Define the path to the Crema folder\n",
    "crema_folder_path = os.path.join(extracted_folder_path, 'Crema')\n",
    "\n",
    "# Verify that we can access the files and extract emotion labels\n",
    "data = []\n",
    "\n",
    "# Loop through each file in the Crema folder\n",
    "for file_name in os.listdir(crema_folder_path):\n",
    "    if file_name.endswith('.wav'):\n",
    "        # Extract the emotion label from the filename\n",
    "        parts = file_name.split('_')\n",
    "        emotion_code = parts[2]\n",
    "        \n",
    "        # Map the emotion code to the actual emotion label\n",
    "        emotion_map = {\n",
    "            'SAD': 'sadness',\n",
    "            'ANG': 'angry',\n",
    "            'DIS': 'disgust',\n",
    "            'FEA': 'fear',\n",
    "            'HAP': 'happy',\n",
    "            'NEU': 'neutral'\n",
    "        }\n",
    "        emotion_label = emotion_map.get(emotion_code, 'unknown')\n",
    "        \n",
    "        # Store the data with the directory path minus the filename\n",
    "        data.append({'filename': file_name, 'emotion': emotion_label, 'path': crema_folder_path})\n",
    "\n",
    "# Convert the data into a DataFrame for easy access\n",
    "df_crema = pd.DataFrame(data)\n",
    "\n",
    "# Display the first few rows to verify\n",
    "print(df_crema.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6c9add4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             filename emotion                    path\n",
      "0  OAF_back_angry.wav   angry  dataset\\Tess\\OAF_angry\n",
      "1   OAF_bar_angry.wav   angry  dataset\\Tess\\OAF_angry\n",
      "2  OAF_base_angry.wav   angry  dataset\\Tess\\OAF_angry\n",
      "3  OAF_bath_angry.wav   angry  dataset\\Tess\\OAF_angry\n",
      "4  OAF_bean_angry.wav   angry  dataset\\Tess\\OAF_angry\n"
     ]
    }
   ],
   "source": [
    "# Define the path to the Tess folder\n",
    "tess_folder_path = os.path.join(extracted_folder_path, 'Tess')\n",
    "\n",
    "# Prepare to store the data\n",
    "data = []\n",
    "\n",
    "# Loop through each emotion folder in the Tess directory\n",
    "for emotion_folder in os.listdir(tess_folder_path):\n",
    "    # Get the full path to the emotion folder\n",
    "    emotion_folder_path = os.path.join(tess_folder_path, emotion_folder)\n",
    "    \n",
    "    # Extract the emotion from the folder name (e.g., \"OAF_angry\" -> \"angry\")\n",
    "    emotion_label = emotion_folder.split('_')[1]\n",
    "    \n",
    "    # Loop through each file in the emotion folder\n",
    "    for file_name in os.listdir(emotion_folder_path):\n",
    "        if file_name.endswith('.wav'):\n",
    "            # Store the data with the directory path minus the filename\n",
    "            data.append({\n",
    "                'filename': file_name, \n",
    "                'emotion': emotion_label, \n",
    "                'path': emotion_folder_path\n",
    "            })\n",
    "\n",
    "# Convert the data into a DataFrame for easy access\n",
    "df_tess = pd.DataFrame(data)\n",
    "\n",
    "# Display the first few rows to verify\n",
    "print(df_tess.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6ce84f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     filename  emotion           path\n",
      "0  DC_a01.wav  unknown  dataset\\Savee\n",
      "1  DC_a02.wav  unknown  dataset\\Savee\n",
      "2  DC_a03.wav  unknown  dataset\\Savee\n",
      "3  DC_a04.wav  unknown  dataset\\Savee\n",
      "4  DC_a05.wav  unknown  dataset\\Savee\n"
     ]
    }
   ],
   "source": [
    "# Define the path to the Savee folder\n",
    "savee_folder_path = os.path.join(extracted_folder_path, 'Savee')\n",
    "\n",
    "# Prepare to store the data\n",
    "data = []\n",
    "\n",
    "# Define the emotion mapping based on the prefixes\n",
    "emotion_map = {\n",
    "    'a': 'anger',\n",
    "    'd': 'disgust',\n",
    "    'f': 'fear',\n",
    "    'h': 'happiness',\n",
    "    'n': 'neutral',\n",
    "    'sa': 'sadness',\n",
    "    'su': 'surprise'\n",
    "}\n",
    "\n",
    "# Loop through each file in the Savee folder\n",
    "for file_name in os.listdir(savee_folder_path):\n",
    "    if file_name.endswith('.wav'):\n",
    "        # Extract the prefix from the filename to determine the emotion\n",
    "        prefix = file_name.split('_')[1][:2]\n",
    "        \n",
    "        # Map the prefix to the corresponding emotion\n",
    "        emotion_label = emotion_map.get(prefix, 'unknown')\n",
    "        \n",
    "        # Store the data with the directory path minus the filename\n",
    "        data.append({\n",
    "            'filename': file_name, \n",
    "            'emotion': emotion_label, \n",
    "            'path': savee_folder_path\n",
    "        })\n",
    "\n",
    "# Convert the data into a DataFrame for easy access\n",
    "df_savee = pd.DataFrame(data)\n",
    "\n",
    "# Display the first few rows to verify\n",
    "print(df_savee.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2423a589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   filename  emotion  \\\n",
      "0  03-01-01-01-01-01-01.wav  neutral   \n",
      "1  03-01-01-01-01-02-01.wav  neutral   \n",
      "2  03-01-01-01-02-01-01.wav  neutral   \n",
      "3  03-01-01-01-02-02-01.wav  neutral   \n",
      "4  03-01-02-01-01-01-01.wav     calm   \n",
      "\n",
      "                                                path  \n",
      "0  dataset\\Ravdess\\audio_speech_actors_01-24\\Acto...  \n",
      "1  dataset\\Ravdess\\audio_speech_actors_01-24\\Acto...  \n",
      "2  dataset\\Ravdess\\audio_speech_actors_01-24\\Acto...  \n",
      "3  dataset\\Ravdess\\audio_speech_actors_01-24\\Acto...  \n",
      "4  dataset\\Ravdess\\audio_speech_actors_01-24\\Acto...  \n"
     ]
    }
   ],
   "source": [
    "# Define the path to the Ravdess folder\n",
    "ravdess_folder_path = os.path.join(extracted_folder_path, 'Ravdess', 'audio_speech_actors_01-24')\n",
    "\n",
    "# Prepare to store the data\n",
    "data = []\n",
    "\n",
    "# Define the emotion mapping based on the third component in the filename\n",
    "emotion_map = {\n",
    "    '01': 'neutral',\n",
    "    '02': 'calm',\n",
    "    '03': 'happy',\n",
    "    '04': 'sad',\n",
    "    '05': 'angry',\n",
    "    '06': 'fearful',\n",
    "    '07': 'disgust',\n",
    "    '08': 'surprised'\n",
    "}\n",
    "\n",
    "# Loop through each actor's folder in the Ravdess directory\n",
    "for actor_folder in os.listdir(ravdess_folder_path):\n",
    "    actor_folder_path = os.path.join(ravdess_folder_path, actor_folder)\n",
    "    \n",
    "    # Loop through each file in the actor's folder\n",
    "    for file_name in os.listdir(actor_folder_path):\n",
    "        if file_name.endswith('.wav'):\n",
    "            # Extract the third component from the filename to determine the emotion\n",
    "            emotion_code = file_name.split('-')[2]\n",
    "            \n",
    "            # Map the emotion code to the corresponding emotion label\n",
    "            emotion_label = emotion_map.get(emotion_code, 'unknown')\n",
    "            \n",
    "            # Store the data with the directory path minus the filename\n",
    "            data.append({\n",
    "                'filename': file_name, \n",
    "                'emotion': emotion_label, \n",
    "                'path': actor_folder_path\n",
    "            })\n",
    "\n",
    "# Convert the data into a DataFrame for easy access\n",
    "df_ravdess = pd.DataFrame(data)\n",
    "\n",
    "# Display the first few rows to verify\n",
    "print(df_ravdess.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a697a74b",
   "metadata": {},
   "source": [
    "### Combining datasets \n",
    "\n",
    "We will merge the datsets into one dataframe, and assign unique identifiers\n",
    "- Concatenate the DataFrames for each dataset.\n",
    "- Assign a unique ID to each entry based on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "aa299939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       id             filename  emotion           path\n",
      "0  c_0001  1001_DFA_ANG_XX.wav    angry  dataset\\Crema\n",
      "1  c_0002  1001_DFA_DIS_XX.wav  disgust  dataset\\Crema\n",
      "2  c_0003  1001_DFA_FEA_XX.wav     fear  dataset\\Crema\n",
      "3  c_0004  1001_DFA_HAP_XX.wav    happy  dataset\\Crema\n",
      "4  c_0005  1001_DFA_NEU_XX.wav  neutral  dataset\\Crema\n"
     ]
    }
   ],
   "source": [
    "# Add a unique ID column to each dataset\n",
    "df_crema['id'] = ['c_{:04d}'.format(i + 1) for i in range(len(df_crema))]\n",
    "df_tess['id'] = ['t_{:04d}'.format(i + 1) for i in range(len(df_tess))]\n",
    "df_savee['id'] = ['s_{:04d}'.format(i + 1) for i in range(len(df_savee))]\n",
    "df_ravdess['id'] = ['r_{:04d}'.format(i + 1) for i in range(len(df_ravdess))]\n",
    "\n",
    "# Merge the datasets into a single DataFrame\n",
    "merged_data = pd.concat([df_crema, df_tess, df_savee, df_ravdess], ignore_index=True)\n",
    "\n",
    "# Reorder columns to have 'id' as the first column\n",
    "merged_data = merged_data[['id', 'filename', 'emotion', 'path']]\n",
    "\n",
    "# Display the first few rows of the combined DataFrame\n",
    "print(merged_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fa45c918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows in dataset: 12162\n"
     ]
    }
   ],
   "source": [
    "# remember, we need at least 1000 rows to meet our requirements. \n",
    "print(f\"Total rows in dataset: {merged_data.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e8a32e",
   "metadata": {},
   "source": [
    "### Extracting Features\n",
    "\n",
    "Again, these are the features we will extract:\n",
    "\n",
    "When extracting the mfcc feature, it may be more usefull to use the MFCC + Δ + ΔΔ approach - which is use when you need to capture dynamic changes in the audio (e.g., speech, music).\n",
    "It's more powerful but adds complexity and is suited for tasks where the temporal evolution of features is important (such as this one!)\n",
    "\n",
    "The idea here is to capture the evolution of the voice (e.g., how pitch, tone, and energy change over time), which is crucial for detecting emotions like happiness, sadness, anger, etc. Let's create a more refined feature extraction pipeline that includes MFCCs, Δ (delta), and ΔΔ (delta-delta) coefficients.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "We will be using the `librosa` package to process these audio features. [Here](https://librosa.org/doc/latest/index.html) is a link to the librosa documentation.\n",
    "\n",
    "**Note**: adding suppression for *UserWarning: Trying to estimate tuning from empty frequency set*. This is likely do to either:* **silence / low energy** (too quiet to perform reliable pitch estimation), or the file had **too short of a duration**. This warning shows up even when setting the pitch to 0 in this case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "00812ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mfcc_with_deltas(y, sr, frame_size, hop_length, n_mfcc=13):\n",
    "    \"\"\"\n",
    "    Extract MFCC features along with delta and delta-delta coefficients.\n",
    "    \n",
    "    Parameters:\n",
    "    - y: Audio time series\n",
    "    - sr: Sample rate of the audio\n",
    "    - frame_size: Frame size for STFT and feature calculations\n",
    "    - hop_length: Hop length for STFT and feature calculations\n",
    "    - n_mfcc: Number of MFCCs to extract\n",
    "    \n",
    "    Returns:\n",
    "    - mfcc_features: Concatenated array of mean MFCC, delta, and delta-delta features\n",
    "    \"\"\"\n",
    "    # Extract MFCCs\n",
    "    mfcc_spec = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc, hop_length=hop_length, n_fft=frame_size)\n",
    "    \n",
    "    # Δ (Delta) coefficients: First-order derivative of MFCCs\n",
    "    delta_mfcc = librosa.feature.delta(mfcc_spec, order=1)\n",
    "    \n",
    "    # ΔΔ (Delta-Delta) coefficients: Second-order derivative of MFCCs\n",
    "    delta2_mfcc = librosa.feature.delta(mfcc_spec, order=2)\n",
    "    \n",
    "    # Compute the mean of MFCC, Δ, and ΔΔ across time (axis=1 is the time axis)\n",
    "    mfcc_features = np.concatenate((\n",
    "        np.mean(mfcc_spec, axis=1),      # Static MFCCs\n",
    "        np.mean(delta_mfcc, axis=1),     # First-order derivatives (Δ)\n",
    "        np.mean(delta2_mfcc, axis=1)     # Second-order derivatives (ΔΔ)\n",
    "    ))\n",
    "    \n",
    "    return mfcc_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "abfd3d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(file_path, frame_size=2048, hop_len=512, mfcc_num=13, sr=22050, n_mels=128):\n",
    "    \"\"\"\n",
    "    Extract features from an audio file including MFCC + Δ + ΔΔ and additional spectral features.\n",
    "    \n",
    "    Returns:\n",
    "    - mfcc: MFCCs (n_mfcc, time)\n",
    "    - spectral_centroid_stats: Tuple containing mean, std, skewness, and kurtosis of spectral centroid\n",
    "    - chroma: Mean chroma feature\n",
    "    - zcr: Mean zero-crossing rate\n",
    "    - rms: Mean RMS energy\n",
    "    - pitch: Mean pitch (using librosa's pitch estimation)\n",
    "    \"\"\"\n",
    "    # Load the audio file\n",
    "    y, sr = librosa.load(file_path, sr=sr)\n",
    "    \n",
    "    # Extract MFCC (with deltas)\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=mfcc_num, hop_length=hop_len, n_fft=frame_size)\n",
    "    \n",
    "    # Spectral centroid (mean, std, skewness, kurtosis)\n",
    "    spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr, hop_length=hop_len)[0]\n",
    "    spectral_centroid_stats = (\n",
    "        np.mean(spectral_centroid), \n",
    "        np.std(spectral_centroid), \n",
    "        skew(spectral_centroid), \n",
    "        kurtosis(spectral_centroid)\n",
    "    )\n",
    "    \n",
    "    # Chroma feature (mean)\n",
    "    chroma = np.mean(librosa.feature.chroma_stft(y=y, sr=sr, hop_length=hop_len))\n",
    "    \n",
    "    # Zero-crossing rate (mean)\n",
    "    zcr = np.mean(librosa.feature.zero_crossing_rate(y, frame_length=frame_size, hop_length=hop_len)[0])\n",
    "    \n",
    "    # RMS energy (mean)\n",
    "    rms = np.mean(librosa.feature.rms(y=y, frame_length=frame_size, hop_length=hop_len)[0])\n",
    "    \n",
    "    # Pitch (using librosa's pitch estimation)\n",
    "    pitches, _ = librosa.piptrack(y=y, sr=sr, n_fft=frame_size, hop_length=hop_len)\n",
    "    pitch = np.mean(pitches[pitches > 0]) if np.any(pitches > 0) else 0  # Take mean pitch if any\n",
    "    \n",
    "    return mfcc, spectral_centroid_stats, chroma, zcr, rms, pitch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d588938",
   "metadata": {},
   "source": [
    "### Validating the Values:\n",
    "\n",
    "- **MFCCs:** Typically, MFCC values range from -400 to 400, depending on the scale of the input signal.\n",
    "> All values: **pass**\n",
    "\n",
    "- **Spectral Centroid:** This value represents the \"center of mass\" of the spectrum and typically ranges between 0 and the - Nyquist frequency (half the sampling rate).\n",
    "> 1584.99: **pass**\n",
    "\n",
    "- **Chroma Features:** These represent the energy distribution across 12 pitch classes. They are normalized, so values between 0 and 1 are expected.\n",
    "> All values: **pass**\n",
    "\n",
    "- **Zero-Crossing Rate:** This rate indicates how frequently the signal changes sign. It ranges from 0 to 1. \n",
    "> 0.1018: **pass**\n",
    "\n",
    "- **RMS Energy:** This value should be within the range of 0 to 1 for normalized signals.\n",
    "> 0.0419: **pass**\n",
    "\n",
    "- **Pitch:** Pitch values are measured in Hz, and depends on the type of audio.\n",
    "> 1211.95: **pass**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb928043",
   "metadata": {},
   "source": [
    "Now that we've validated our extract_features function, we can apply it to the rest of our dataframe.\n",
    "\n",
    "**Notes**: \n",
    "- This cell can take a while to run! About 5 minutes\n",
    "- suppressed UserWarning: Trying to estimate tuning from empty frequency set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "618b2c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\Anaconda\\envs\\learn-env\\lib\\site-packages\\librosa\\core\\pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n",
      "  return pitch_tuning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                            0\n",
      "filename                      0\n",
      "emotion                       0\n",
      "path                          0\n",
      "mfccs_mean                    0\n",
      "mfccs_std                     0\n",
      "mfccs_skewness                0\n",
      "mfccs_kurtosis                0\n",
      "spectral_centroid             0\n",
      "spectral_centroid_std         0\n",
      "spectral_centroid_skewness    0\n",
      "spectral_centroid_kurtosis    0\n",
      "chroma                        0\n",
      "zero_crossing_rate            0\n",
      "rms                           0\n",
      "pitch                         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Empty lists to store features\n",
    "mfccs_mean_list = []\n",
    "mfccs_std_list = []\n",
    "mfccs_skewness_list = []\n",
    "mfccs_kurtosis_list = []\n",
    "\n",
    "spectral_centroid_list = []\n",
    "spectral_centroid_std_list = []\n",
    "spectral_centroid_skewness_list = []\n",
    "spectral_centroid_kurtosis_list = []\n",
    "\n",
    "chroma_list = []\n",
    "zero_crossing_rate_list = []\n",
    "rms_list = []\n",
    "pitch_list = []\n",
    "\n",
    "# Assuming 'merged_data' DataFrame has the columns 'path' and 'filename'\n",
    "for index, row in merged_data.iterrows():\n",
    "    file_path = os.path.join(row['path'], row['filename'])  # Construct the file path\n",
    "    \n",
    "    # Extract features directly from the file path\n",
    "    features = extract_features(file_path)\n",
    "    \n",
    "    # Extract MFCCs time-series (n_mfcc, time)\n",
    "    mfccs = features[0]  # MFCC time-series (shape: n_mfcc, time)\n",
    "    \n",
    "    # Compute MFCC summary statistics for each coefficient\n",
    "    mfccs_mean = np.mean(mfccs, axis=1)  # Mean of each MFCC coefficient (shape: n_mfcc)\n",
    "    mfccs_std = np.std(mfccs, axis=1)    # Standard deviation of each MFCC coefficient\n",
    "    mfccs_skewness = skew(mfccs, axis=1)  # Skewness of each MFCC coefficient\n",
    "    mfccs_kurtosis = kurtosis(mfccs, axis=1)  # Kurtosis of each MFCC coefficient\n",
    "\n",
    "    # Append MFCC summary statistics to the lists\n",
    "    mfccs_mean_list.append(mfccs_mean)\n",
    "    mfccs_std_list.append(mfccs_std)\n",
    "    mfccs_skewness_list.append(mfccs_skewness)\n",
    "    mfccs_kurtosis_list.append(mfccs_kurtosis)\n",
    "    \n",
    "    # Extract Spectral Centroid (mean, std, skewness, kurtosis)\n",
    "    spectral_centroid_list.append(features[1][0])  # Spectral centroid mean\n",
    "    spectral_centroid_std_list.append(features[1][1])  # Spectral centroid std\n",
    "    spectral_centroid_skewness_list.append(features[1][2])  # Spectral centroid skewness\n",
    "    spectral_centroid_kurtosis_list.append(features[1][3])  # Spectral centroid kurtosis\n",
    "\n",
    "    # Extract other features\n",
    "    chroma_list.append(features[2])  # Chroma feature\n",
    "    zero_crossing_rate_list.append(features[3])  # Zero-crossing rate\n",
    "    rms_list.append(features[4])  # RMS energy\n",
    "    pitch_list.append(features[5])  # Pitch\n",
    "\n",
    "# Add the features to the DataFrame\n",
    "\n",
    "# MFCCs are stored as lists of arrays; handle them carefully\n",
    "merged_data['mfccs_mean'] = mfccs_mean_list\n",
    "merged_data['mfccs_std'] = mfccs_std_list\n",
    "merged_data['mfccs_skewness'] = mfccs_skewness_list\n",
    "merged_data['mfccs_kurtosis'] = mfccs_kurtosis_list\n",
    "\n",
    "# Spectral Centroid features\n",
    "merged_data['spectral_centroid'] = spectral_centroid_list\n",
    "merged_data['spectral_centroid_std'] = spectral_centroid_std_list\n",
    "merged_data['spectral_centroid_skewness'] = spectral_centroid_skewness_list\n",
    "merged_data['spectral_centroid_kurtosis'] = spectral_centroid_kurtosis_list\n",
    "\n",
    "# Other features\n",
    "merged_data['chroma'] = chroma_list\n",
    "merged_data['zero_crossing_rate'] = zero_crossing_rate_list\n",
    "merged_data['rms'] = rms_list\n",
    "merged_data['pitch'] = pitch_list\n",
    "\n",
    "# Check for any NaN values in the DataFrame\n",
    "print(merged_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "bd6f6a36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>filename</th>\n",
       "      <th>emotion</th>\n",
       "      <th>path</th>\n",
       "      <th>mfccs_mean</th>\n",
       "      <th>mfccs_std</th>\n",
       "      <th>mfccs_skewness</th>\n",
       "      <th>mfccs_kurtosis</th>\n",
       "      <th>spectral_centroid</th>\n",
       "      <th>spectral_centroid_std</th>\n",
       "      <th>spectral_centroid_skewness</th>\n",
       "      <th>spectral_centroid_kurtosis</th>\n",
       "      <th>chroma</th>\n",
       "      <th>zero_crossing_rate</th>\n",
       "      <th>rms</th>\n",
       "      <th>pitch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c_0001</td>\n",
       "      <td>1001_DFA_ANG_XX.wav</td>\n",
       "      <td>angry</td>\n",
       "      <td>dataset\\Crema</td>\n",
       "      <td>[-341.98892, 120.799965, -9.834449, 39.60623, ...</td>\n",
       "      <td>[142.84297, 28.326567, 47.76834, 20.65179, 24....</td>\n",
       "      <td>[0.2659386, -0.95511705, -0.47179157, 0.901687...</td>\n",
       "      <td>[-1.3775886, 3.0805964, -1.2524763, 0.64642143...</td>\n",
       "      <td>1516.157974</td>\n",
       "      <td>610.822815</td>\n",
       "      <td>1.707807</td>\n",
       "      <td>3.257092</td>\n",
       "      <td>0.423201</td>\n",
       "      <td>0.080581</td>\n",
       "      <td>0.040486</td>\n",
       "      <td>1246.326904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c_0002</td>\n",
       "      <td>1001_DFA_DIS_XX.wav</td>\n",
       "      <td>disgust</td>\n",
       "      <td>dataset\\Crema</td>\n",
       "      <td>[-388.46194, 133.88211, -17.753077, 53.109615,...</td>\n",
       "      <td>[107.1445, 21.450508, 38.543686, 17.076757, 16...</td>\n",
       "      <td>[0.48839697, -1.0026201, -0.5151938, 1.4708819...</td>\n",
       "      <td>[-1.3783861, 4.694527, -1.283674, 2.1692944, 0...</td>\n",
       "      <td>1457.589670</td>\n",
       "      <td>581.391412</td>\n",
       "      <td>2.228195</td>\n",
       "      <td>6.003036</td>\n",
       "      <td>0.423743</td>\n",
       "      <td>0.069094</td>\n",
       "      <td>0.015566</td>\n",
       "      <td>1265.043457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c_0003</td>\n",
       "      <td>1001_DFA_FEA_XX.wav</td>\n",
       "      <td>fear</td>\n",
       "      <td>dataset\\Crema</td>\n",
       "      <td>[-352.20844, 115.79107, -3.4151676, 32.881832,...</td>\n",
       "      <td>[146.9967, 18.543854, 48.12259, 15.59467, 26.0...</td>\n",
       "      <td>[0.65639275, 0.14103335, -0.7013156, 1.3434969...</td>\n",
       "      <td>[-1.3745373, 1.62991, -1.0806166, 3.2819757, -...</td>\n",
       "      <td>1421.898267</td>\n",
       "      <td>514.679124</td>\n",
       "      <td>2.811464</td>\n",
       "      <td>10.612630</td>\n",
       "      <td>0.418467</td>\n",
       "      <td>0.062765</td>\n",
       "      <td>0.044071</td>\n",
       "      <td>998.714294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>c_0004</td>\n",
       "      <td>1001_DFA_HAP_XX.wav</td>\n",
       "      <td>happy</td>\n",
       "      <td>dataset\\Crema</td>\n",
       "      <td>[-337.83957, 120.109604, -12.691747, 39.687584...</td>\n",
       "      <td>[138.8324, 21.34765, 42.51781, 18.510075, 26.9...</td>\n",
       "      <td>[0.42359322, 0.17657933, -0.57947487, 1.164496...</td>\n",
       "      <td>[-1.5084553, 1.193562, -1.1124841, 1.246069, -...</td>\n",
       "      <td>1481.793208</td>\n",
       "      <td>485.050603</td>\n",
       "      <td>2.844557</td>\n",
       "      <td>10.717863</td>\n",
       "      <td>0.393811</td>\n",
       "      <td>0.063893</td>\n",
       "      <td>0.040840</td>\n",
       "      <td>1108.554565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c_0005</td>\n",
       "      <td>1001_DFA_NEU_XX.wav</td>\n",
       "      <td>neutral</td>\n",
       "      <td>dataset\\Crema</td>\n",
       "      <td>[-368.17557, 124.36494, -4.6439185, 39.390568,...</td>\n",
       "      <td>[95.985146, 23.687752, 34.455185, 16.717134, 1...</td>\n",
       "      <td>[0.456242, 0.036559895, -0.38090122, 1.2902434...</td>\n",
       "      <td>[-1.1498214, 0.6163354, -1.0719036, 1.5027843,...</td>\n",
       "      <td>1424.198808</td>\n",
       "      <td>505.258131</td>\n",
       "      <td>1.782787</td>\n",
       "      <td>3.785639</td>\n",
       "      <td>0.408446</td>\n",
       "      <td>0.061546</td>\n",
       "      <td>0.019763</td>\n",
       "      <td>1062.882446</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id             filename  emotion           path  \\\n",
       "0  c_0001  1001_DFA_ANG_XX.wav    angry  dataset\\Crema   \n",
       "1  c_0002  1001_DFA_DIS_XX.wav  disgust  dataset\\Crema   \n",
       "2  c_0003  1001_DFA_FEA_XX.wav     fear  dataset\\Crema   \n",
       "3  c_0004  1001_DFA_HAP_XX.wav    happy  dataset\\Crema   \n",
       "4  c_0005  1001_DFA_NEU_XX.wav  neutral  dataset\\Crema   \n",
       "\n",
       "                                          mfccs_mean  \\\n",
       "0  [-341.98892, 120.799965, -9.834449, 39.60623, ...   \n",
       "1  [-388.46194, 133.88211, -17.753077, 53.109615,...   \n",
       "2  [-352.20844, 115.79107, -3.4151676, 32.881832,...   \n",
       "3  [-337.83957, 120.109604, -12.691747, 39.687584...   \n",
       "4  [-368.17557, 124.36494, -4.6439185, 39.390568,...   \n",
       "\n",
       "                                           mfccs_std  \\\n",
       "0  [142.84297, 28.326567, 47.76834, 20.65179, 24....   \n",
       "1  [107.1445, 21.450508, 38.543686, 17.076757, 16...   \n",
       "2  [146.9967, 18.543854, 48.12259, 15.59467, 26.0...   \n",
       "3  [138.8324, 21.34765, 42.51781, 18.510075, 26.9...   \n",
       "4  [95.985146, 23.687752, 34.455185, 16.717134, 1...   \n",
       "\n",
       "                                      mfccs_skewness  \\\n",
       "0  [0.2659386, -0.95511705, -0.47179157, 0.901687...   \n",
       "1  [0.48839697, -1.0026201, -0.5151938, 1.4708819...   \n",
       "2  [0.65639275, 0.14103335, -0.7013156, 1.3434969...   \n",
       "3  [0.42359322, 0.17657933, -0.57947487, 1.164496...   \n",
       "4  [0.456242, 0.036559895, -0.38090122, 1.2902434...   \n",
       "\n",
       "                                      mfccs_kurtosis  spectral_centroid  \\\n",
       "0  [-1.3775886, 3.0805964, -1.2524763, 0.64642143...        1516.157974   \n",
       "1  [-1.3783861, 4.694527, -1.283674, 2.1692944, 0...        1457.589670   \n",
       "2  [-1.3745373, 1.62991, -1.0806166, 3.2819757, -...        1421.898267   \n",
       "3  [-1.5084553, 1.193562, -1.1124841, 1.246069, -...        1481.793208   \n",
       "4  [-1.1498214, 0.6163354, -1.0719036, 1.5027843,...        1424.198808   \n",
       "\n",
       "   spectral_centroid_std  spectral_centroid_skewness  \\\n",
       "0             610.822815                    1.707807   \n",
       "1             581.391412                    2.228195   \n",
       "2             514.679124                    2.811464   \n",
       "3             485.050603                    2.844557   \n",
       "4             505.258131                    1.782787   \n",
       "\n",
       "   spectral_centroid_kurtosis    chroma  zero_crossing_rate       rms  \\\n",
       "0                    3.257092  0.423201            0.080581  0.040486   \n",
       "1                    6.003036  0.423743            0.069094  0.015566   \n",
       "2                   10.612630  0.418467            0.062765  0.044071   \n",
       "3                   10.717863  0.393811            0.063893  0.040840   \n",
       "4                    3.785639  0.408446            0.061546  0.019763   \n",
       "\n",
       "         pitch  \n",
       "0  1246.326904  \n",
       "1  1265.043457  \n",
       "2   998.714294  \n",
       "3  1108.554565  \n",
       "4  1062.882446  "
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_data['pitch'] = merged_data['pitch'].fillna(0)\n",
    "merged_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d19aa0e",
   "metadata": {},
   "source": [
    "We won't be able to work with arrays - we will need to extract a meaningful metric and save them in a new column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb91387",
   "metadata": {},
   "source": [
    "### Cleaning the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "73c2f77c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   mfccs_mean  mfccs_std  mfccs_skewness  mfccs_kurtosis  \\\n",
      "0  -17.817301  35.118603        0.469488        0.654481   \n",
      "1  -18.545635  26.126188        0.571953        0.562407   \n",
      "2  -18.392933  36.777905        0.290787       -1.264621   \n",
      "3  -18.045368  34.028591        0.291041       -0.969027   \n",
      "4  -18.101955  22.763020        0.121927       -0.931959   \n",
      "\n",
      "   spectral_centroid_mean  spectral_centroid_std  spectral_centroid_skewness  \\\n",
      "0             1516.157974             610.822815                    1.707807   \n",
      "1             1457.589670             581.391412                    2.228195   \n",
      "2             1421.898267             514.679124                    2.811464   \n",
      "3             1481.793208             485.050603                    2.844557   \n",
      "4             1424.198808             505.258131                    1.782787   \n",
      "\n",
      "   spectral_centroid_kurtosis  chroma_mean  chroma_std  ...  \\\n",
      "0                    3.257092          NaN         NaN  ...   \n",
      "1                    6.003036          NaN         NaN  ...   \n",
      "2                   10.612630          NaN         NaN  ...   \n",
      "3                   10.717863          NaN         NaN  ...   \n",
      "4                    3.785639          NaN         NaN  ...   \n",
      "\n",
      "   zero_crossing_rate_skewness  zero_crossing_rate_kurtosis  rms_mean  \\\n",
      "0                          NaN                          NaN  0.040486   \n",
      "1                          NaN                          NaN  0.015566   \n",
      "2                          NaN                          NaN  0.044071   \n",
      "3                          NaN                          NaN  0.040840   \n",
      "4                          NaN                          NaN  0.019763   \n",
      "\n",
      "   rms_std  rms_skewness  rms_kurtosis        pitch           path  \\\n",
      "0      NaN           NaN           NaN  1246.326904  dataset\\Crema   \n",
      "1      NaN           NaN           NaN  1265.043457  dataset\\Crema   \n",
      "2      NaN           NaN           NaN   998.714294  dataset\\Crema   \n",
      "3      NaN           NaN           NaN  1108.554565  dataset\\Crema   \n",
      "4      NaN           NaN           NaN  1062.882446  dataset\\Crema   \n",
      "\n",
      "              filename  emotion  \n",
      "0  1001_DFA_ANG_XX.wav    angry  \n",
      "1  1001_DFA_DIS_XX.wav  disgust  \n",
      "2  1001_DFA_FEA_XX.wav     fear  \n",
      "3  1001_DFA_HAP_XX.wav    happy  \n",
      "4  1001_DFA_NEU_XX.wav  neutral  \n",
      "\n",
      "[5 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "# Create a new DataFrame for the cleaned features\n",
    "clean_data = pd.DataFrame()\n",
    "\n",
    "# Store MFCC summary statistics (mean, std, skewness, kurtosis)\n",
    "clean_data['mfccs_mean'] = merged_data['mfccs_mean'].apply(lambda x: np.mean(x) if isinstance(x, (list, np.ndarray)) and len(x) > 0 else np.nan)\n",
    "clean_data['mfccs_std'] = merged_data['mfccs_std'].apply(lambda x: np.std(x) if isinstance(x, (list, np.ndarray)) and len(x) > 0 else np.nan)\n",
    "clean_data['mfccs_skewness'] = merged_data['mfccs_skewness'].apply(lambda x: skew(x) if isinstance(x, (list, np.ndarray)) and len(x) > 0 else np.nan)\n",
    "clean_data['mfccs_kurtosis'] = merged_data['mfccs_kurtosis'].apply(lambda x: kurtosis(x) if isinstance(x, (list, np.ndarray)) and len(x) > 0 else np.nan)\n",
    "\n",
    "# Store the statistics for Spectral Centroid (these are already individual numbers)\n",
    "clean_data['spectral_centroid_mean'] = merged_data['spectral_centroid'].apply(lambda x: x if isinstance(x, (float, int)) else np.nan)\n",
    "clean_data['spectral_centroid_std'] = merged_data['spectral_centroid_std'].apply(lambda x: x if isinstance(x, (float, int)) else np.nan)\n",
    "clean_data['spectral_centroid_skewness'] = merged_data['spectral_centroid_skewness'].apply(lambda x: x if isinstance(x, (float, int)) else np.nan)\n",
    "clean_data['spectral_centroid_kurtosis'] = merged_data['spectral_centroid_kurtosis'].apply(lambda x: x if isinstance(x, (float, int)) else np.nan)\n",
    "\n",
    "# More robust handling for Chroma: Handle missing, empty, or scalar cases\n",
    "def compute_chroma_statistics(chroma_feature):\n",
    "    if isinstance(chroma_feature, (list, np.ndarray)) and len(chroma_feature) > 0:\n",
    "        return {\n",
    "            'mean': np.mean(chroma_feature),\n",
    "            'std': np.std(chroma_feature) if len(chroma_feature) > 1 else np.nan,\n",
    "            'skewness': skew(chroma_feature) if len(chroma_feature) > 1 else np.nan,\n",
    "            'kurtosis': kurtosis(chroma_feature) if len(chroma_feature) > 1 else np.nan\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            'mean': np.nan,\n",
    "            'std': np.nan,\n",
    "            'skewness': np.nan,\n",
    "            'kurtosis': np.nan\n",
    "        }\n",
    "\n",
    "chroma_stats = merged_data['chroma'].apply(compute_chroma_statistics)\n",
    "\n",
    "clean_data['chroma_mean'] = chroma_stats.apply(lambda x: x['mean'])\n",
    "clean_data['chroma_std'] = chroma_stats.apply(lambda x: x['std'])\n",
    "clean_data['chroma_skewness'] = chroma_stats.apply(lambda x: x['skewness'])\n",
    "clean_data['chroma_kurtosis'] = chroma_stats.apply(lambda x: x['kurtosis'])\n",
    "\n",
    "# More robust handling for Zero-Crossing Rate: Handle missing, empty, or scalar cases\n",
    "def compute_zero_crossing_statistics(zcr):\n",
    "    if isinstance(zcr, (list, np.ndarray)) and len(zcr) > 0:\n",
    "        return {\n",
    "            'mean': np.mean(zcr),\n",
    "            'std': np.std(zcr) if len(zcr) > 1 else np.nan,\n",
    "            'skewness': skew(zcr) if len(zcr) > 1 else np.nan,\n",
    "            'kurtosis': kurtosis(zcr) if len(zcr) > 1 else np.nan\n",
    "        }\n",
    "    elif isinstance(zcr, (float, int)):\n",
    "        return {\n",
    "            'mean': zcr,\n",
    "            'std': np.nan,\n",
    "            'skewness': np.nan,\n",
    "            'kurtosis': np.nan\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            'mean': np.nan,\n",
    "            'std': np.nan,\n",
    "            'skewness': np.nan,\n",
    "            'kurtosis': np.nan\n",
    "        }\n",
    "\n",
    "zcr_stats = merged_data['zero_crossing_rate'].apply(compute_zero_crossing_statistics)\n",
    "\n",
    "clean_data['zero_crossing_rate_mean'] = zcr_stats.apply(lambda x: x['mean'])\n",
    "clean_data['zero_crossing_rate_std'] = zcr_stats.apply(lambda x: x['std'])\n",
    "clean_data['zero_crossing_rate_skewness'] = zcr_stats.apply(lambda x: x['skewness'])\n",
    "clean_data['zero_crossing_rate_kurtosis'] = zcr_stats.apply(lambda x: x['kurtosis'])\n",
    "\n",
    "# More robust handling for RMS: Handle missing, empty, or scalar cases\n",
    "def compute_rms_statistics(rms):\n",
    "    if isinstance(rms, (list, np.ndarray)) and len(rms) > 0:\n",
    "        return {\n",
    "            'mean': np.mean(rms),\n",
    "            'std': np.std(rms) if len(rms) > 1 else np.nan,\n",
    "            'skewness': skew(rms) if len(rms) > 1 else np.nan,\n",
    "            'kurtosis': kurtosis(rms) if len(rms) > 1 else np.nan\n",
    "        }\n",
    "    elif isinstance(rms, (float, int)):\n",
    "        return {\n",
    "            'mean': rms,\n",
    "            'std': np.nan,\n",
    "            'skewness': np.nan,\n",
    "            'kurtosis': np.nan\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            'mean': np.nan,\n",
    "            'std': np.nan,\n",
    "            'skewness': np.nan,\n",
    "            'kurtosis': np.nan\n",
    "        }\n",
    "\n",
    "rms_stats = merged_data['rms'].apply(compute_rms_statistics)\n",
    "\n",
    "clean_data['rms_mean'] = rms_stats.apply(lambda x: x['mean'])\n",
    "clean_data['rms_std'] = rms_stats.apply(lambda x: x['std'])\n",
    "clean_data['rms_skewness'] = rms_stats.apply(lambda x: x['skewness'])\n",
    "clean_data['rms_kurtosis'] = rms_stats.apply(lambda x: x['kurtosis'])\n",
    "\n",
    "# Directly store the existing pitch and other metadata\n",
    "clean_data['pitch'] = merged_data['pitch']  # Pitch is already a single value\n",
    "clean_data['path'] = merged_data['path']  # Path to the audio file\n",
    "clean_data['filename'] = merged_data['filename']  # Filename of the audio file\n",
    "\n",
    "# Add the emotion column (assuming it's a label in your dataset)\n",
    "clean_data['emotion'] = merged_data['emotion']\n",
    "\n",
    "# Display the new DataFrame\n",
    "print(clean_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "5b42cfd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mfccs_mean                         0\n",
      "mfccs_std                          0\n",
      "mfccs_skewness                     0\n",
      "mfccs_kurtosis                     0\n",
      "spectral_centroid_mean             0\n",
      "spectral_centroid_std              0\n",
      "spectral_centroid_skewness         0\n",
      "spectral_centroid_kurtosis         0\n",
      "chroma_mean                    12162\n",
      "chroma_std                     12162\n",
      "chroma_skewness                12162\n",
      "chroma_kurtosis                12162\n",
      "zero_crossing_rate_mean            0\n",
      "zero_crossing_rate_std         12162\n",
      "zero_crossing_rate_skewness    12162\n",
      "zero_crossing_rate_kurtosis    12162\n",
      "rms_mean                           0\n",
      "rms_std                        12162\n",
      "rms_skewness                   12162\n",
      "rms_kurtosis                   12162\n",
      "pitch                              0\n",
      "path                               0\n",
      "filename                           0\n",
      "emotion                            0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for any NaN values in the DataFrame\n",
    "print(clean_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "e89ac408",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# distribution of emotions\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=clean_data, x='emotion', order=clean_data['emotion'].value_counts().index)\n",
    "plt.title('Distribution of Emotions')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# this chart shows some a significant imbalance in the dataset. lets do some resampling\n",
    "# Lets try using the SMOTE technique (Synthetic Minority Over-sampling):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "b6a2bfbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique emotions after mapping: ['angry' 'disgust' 'fear' 'happy' 'neutral' 'sad' 'unknown' 'surprise']\n"
     ]
    }
   ],
   "source": [
    "# Convert all emotion labels to lowercase to standardize\n",
    "clean_data['emotion'] = clean_data['emotion'].str.lower()\n",
    "\n",
    "# Map similar emotions to a single label\n",
    "clean_data['emotion'] = clean_data['emotion'].replace({\n",
    "    'surprised': 'surprise',\n",
    "    'sadness': 'sad',\n",
    "    'fearful': 'fear',\n",
    "    'pleasant': 'happy',\n",
    "    'calm': 'neutral',\n",
    "    'surprise': 'fear'        \n",
    "})\n",
    "\n",
    "# Check the unique values after standardization\n",
    "unique_emotions = clean_data['emotion'].unique()\n",
    "print(\"Unique emotions after mapping:\", unique_emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "762ac10e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=clean_data, x='emotion', order=clean_data['emotion'].value_counts().index)\n",
    "plt.title('Distribution of Emotions')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "20f5327f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mfccs_mean</th>\n",
       "      <th>mfccs_std</th>\n",
       "      <th>mfccs_skewness</th>\n",
       "      <th>mfccs_kurtosis</th>\n",
       "      <th>spectral_centroid_mean</th>\n",
       "      <th>spectral_centroid_std</th>\n",
       "      <th>spectral_centroid_skewness</th>\n",
       "      <th>spectral_centroid_kurtosis</th>\n",
       "      <th>chroma</th>\n",
       "      <th>zero_crossing_rate</th>\n",
       "      <th>rms</th>\n",
       "      <th>pitch</th>\n",
       "      <th>path</th>\n",
       "      <th>filename</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-17.963037</td>\n",
       "      <td>26.372070</td>\n",
       "      <td>-0.515480</td>\n",
       "      <td>0.329989</td>\n",
       "      <td>1584.993070</td>\n",
       "      <td>600.410753</td>\n",
       "      <td>1.645968</td>\n",
       "      <td>5.914065</td>\n",
       "      <td>0.411293</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1211.950684</td>\n",
       "      <td>dataset\\Crema</td>\n",
       "      <td>1001_DFA_ANG_XX.wav</td>\n",
       "      <td>angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-18.657297</td>\n",
       "      <td>21.019888</td>\n",
       "      <td>-0.498601</td>\n",
       "      <td>0.982955</td>\n",
       "      <td>1531.650487</td>\n",
       "      <td>590.708457</td>\n",
       "      <td>2.221739</td>\n",
       "      <td>8.718056</td>\n",
       "      <td>0.423961</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1256.617188</td>\n",
       "      <td>dataset\\Crema</td>\n",
       "      <td>1001_DFA_DIS_XX.wav</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-18.552622</td>\n",
       "      <td>25.241508</td>\n",
       "      <td>-0.602645</td>\n",
       "      <td>1.156697</td>\n",
       "      <td>1489.088839</td>\n",
       "      <td>521.794373</td>\n",
       "      <td>2.785179</td>\n",
       "      <td>13.443495</td>\n",
       "      <td>0.413398</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>992.574402</td>\n",
       "      <td>dataset\\Crema</td>\n",
       "      <td>1001_DFA_FEA_XX.wav</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-18.460817</td>\n",
       "      <td>24.993519</td>\n",
       "      <td>-0.653859</td>\n",
       "      <td>0.327673</td>\n",
       "      <td>1555.376035</td>\n",
       "      <td>476.260688</td>\n",
       "      <td>2.604170</td>\n",
       "      <td>11.342148</td>\n",
       "      <td>0.394820</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1102.953003</td>\n",
       "      <td>dataset\\Crema</td>\n",
       "      <td>1001_DFA_HAP_XX.wav</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-18.111607</td>\n",
       "      <td>20.893410</td>\n",
       "      <td>-0.480883</td>\n",
       "      <td>0.167808</td>\n",
       "      <td>1495.394997</td>\n",
       "      <td>492.130906</td>\n",
       "      <td>1.597144</td>\n",
       "      <td>6.070896</td>\n",
       "      <td>0.401279</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1041.093628</td>\n",
       "      <td>dataset\\Crema</td>\n",
       "      <td>1001_DFA_NEU_XX.wav</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mfccs_mean  mfccs_std  mfccs_skewness  mfccs_kurtosis  \\\n",
       "0  -17.963037  26.372070       -0.515480        0.329989   \n",
       "1  -18.657297  21.019888       -0.498601        0.982955   \n",
       "2  -18.552622  25.241508       -0.602645        1.156697   \n",
       "3  -18.460817  24.993519       -0.653859        0.327673   \n",
       "4  -18.111607  20.893410       -0.480883        0.167808   \n",
       "\n",
       "   spectral_centroid_mean  spectral_centroid_std  spectral_centroid_skewness  \\\n",
       "0             1584.993070             600.410753                    1.645968   \n",
       "1             1531.650487             590.708457                    2.221739   \n",
       "2             1489.088839             521.794373                    2.785179   \n",
       "3             1555.376035             476.260688                    2.604170   \n",
       "4             1495.394997             492.130906                    1.597144   \n",
       "\n",
       "   spectral_centroid_kurtosis    chroma  zero_crossing_rate  rms        pitch  \\\n",
       "0                    5.914065  0.411293                 NaN  NaN  1211.950684   \n",
       "1                    8.718056  0.423961                 NaN  NaN  1256.617188   \n",
       "2                   13.443495  0.413398                 NaN  NaN   992.574402   \n",
       "3                   11.342148  0.394820                 NaN  NaN  1102.953003   \n",
       "4                    6.070896  0.401279                 NaN  NaN  1041.093628   \n",
       "\n",
       "            path             filename  emotion  \n",
       "0  dataset\\Crema  1001_DFA_ANG_XX.wav    angry  \n",
       "1  dataset\\Crema  1001_DFA_DIS_XX.wav  disgust  \n",
       "2  dataset\\Crema  1001_DFA_FEA_XX.wav     fear  \n",
       "3  dataset\\Crema  1001_DFA_HAP_XX.wav    happy  \n",
       "4  dataset\\Crema  1001_DFA_NEU_XX.wav  neutral  "
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop rows where the 'emotion' column has the value 'unknown'\n",
    "clean_data = clean_data[clean_data['emotion'] != 'unknown']\n",
    "\n",
    "clean_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df83318",
   "metadata": {},
   "source": [
    "# Model Iterations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9af01b7",
   "metadata": {},
   "source": [
    "#### Evaluation v1:\n",
    "\n",
    "Classification Report (grid search)\n",
    "\n",
    "|               | precision | recall | f1-score | support |\n",
    "|---------------|-----------|--------|----------|---------|\n",
    "| accuracy      |           |        |          | 0.46    | 2433    |\n",
    "| macro avg     | 0.43      | 0.43   | 0.43     | 2433    |\n",
    "| weighted avg  | 0.46      | 0.46   | 0.45     | 2433    |\n",
    "\n",
    "#### Evaluation v2:\n",
    "\n",
    "Classification Report (grid search with SMOTE):\n",
    "\n",
    "|               | precision | recall | f1-score | support |\n",
    "|---------------|-----------|--------|----------|---------|\n",
    "| accuracy      |           |        |          | 0.71    | 5962    |\n",
    "| macro avg     | 0.70      | 0.71   | 0.70     | 5962    |\n",
    "| weighted avg  | 0.70      | 0.71   | 0.71     | 5962    |\n",
    "\n",
    "#### Evaluation v3:\n",
    "\n",
    "Classification Report (decision tree w/ augmentation and 1500 samples per category):\n",
    "\n",
    "|               | precision | recall | f1-score | support |\n",
    "|---------------|-----------|--------|----------|---------|\n",
    "| accuracy      |           |        |          | 0.58    | 2700    |\n",
    "| macro avg     | 0.57      | 0.58   | 0.57     | 2700    |\n",
    "| weighted avg  | 0.57      | 0.58   | 0.57     | 2700    |\n",
    "\n",
    "Accuracy of 59%; certainly an imporovement! But nothing substantial. \n",
    "\n",
    "Despite evening the distribution, we are still not seeing the model perform well. This is likely because 7k records\n",
    "is probably not enough to get good predictive capacity over our 7 categories. \n",
    "\n",
    "the f1-scores that indicate weaknesses in our model is: disgust, fear, happy. this tells me that maybe we shouldn't have deleted \n",
    "\n",
    "Let's engineer the categories into pos/neg binary classification and see if that improves our results.\n",
    "\n",
    "#### Evaluation v4:\n",
    "\n",
    "Classification Report (decision tree w/ even distribution, no deleting records):\n",
    "\n",
    "|               | precision | recall | f1-score | support |\n",
    "|---------------|-----------|--------|----------|---------|\n",
    "| accuracy      |           |        |          | 0.56    | 3135    |\n",
    "| macro avg     | 0.57      | 0.59   | 0.57     | 3135    |\n",
    "| weighted avg  | 0.55      | 0.56   | 0.54     | 3135    |\n",
    "\n",
    "Accuracy of 55.34%: While this may seem like a drop, it's important to note that the dataset has been augmented, which can affect the accuracy score. \n",
    "\n",
    "F1-scores: \n",
    "- the f1-scores for underrepresented classes like 'disgust', 'fear', and 'happy' slightly improved compared to previous version.\n",
    "- the f1-score for 'neutral' decreased slightly (0.02), which could be due to the additional augmented samples\n",
    "- the f1-score for the other classes either remained or improved marginally\n",
    "\n",
    "Precision / Recall:\n",
    "- the recall for underrepresented classes has generally improved\n",
    "- however, the precision (ability to not label negative instances as positive) has decreased, which could be due to the introduction of noise in the augmented data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c934d696",
   "metadata": {},
   "source": [
    "#### Evaluation v5\n",
    "\n",
    "Classification Report (logistic regression for **binary classification**):  \n",
    "\n",
    "| | precision | recall | f1-score | support |\n",
    "|---------------|-----------|--------|----------|---------|\n",
    "| Negative | 0.65 | 0.74 | 0.69 | 1536 |\n",
    "| Positive | 0.36 | 0.27 | 0.31 | 825 |\n",
    "| accuracy | | | | 0.57 | 2361 |\n",
    "| macro avg | 0.51 | 0.51 | 0.50 | 2361 |\n",
    "| weighted avg | 0.55 | 0.57 | 0.56 | 2361 |\n",
    "\n",
    "As suspected, the 'positive' data suffered due to imbalanced classes. \n",
    "\n",
    "Let's augment some of the audio for the 'positive' class, and bring it closer to the negative class balance.\n",
    "\n",
    "#### Evaluation v6\n",
    "Classification Report (logistic regression for **binary classification with augmented data**): \n",
    "\n",
    "| | precision | recall | f1-score | support |\n",
    "|---------------|-----------|--------|----------|---------|\n",
    "| Negative | 0.65 | 0.74 | 0.69 | 1536 |\n",
    "| Positive | 0.36 | 0.27 | 0.31 | 825 |\n",
    "| accuracy | | | | 0.57 | 2361 |\n",
    "| macro avg | 0.51 | 0.51 | 0.50 | 2361 |\n",
    "| weighted avg | 0.55 | 0.57 | 0.56 | 2361 |\n",
    "\n",
    "Having the same values as v5, I believe where we split the data caused data leakage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb8111c",
   "metadata": {},
   "source": [
    "#### Evaluation v7\n",
    "\n",
    "Classification Report (neural network for multiclassification):\n",
    "\n",
    "| Emotion   | Precision | Recall | F1-Score | Support |\n",
    "|-----------|-----------|--------|----------|---------|\n",
    "| angry     | 0.66      | 0.60   | 0.63     | 405     |\n",
    "| calm      | 0.74      | 0.86   | 0.79     | 291     |\n",
    "| disgust   | 0.52      | 0.30   | 0.38     | 383     |\n",
    "| fear      | 0.71      | 0.22   | 0.34     | 388     |\n",
    "| happy     | 0.39      | 0.40   | 0.39     | 364     |\n",
    "| neutral   | 0.42      | 0.57   | 0.49     | 367     |\n",
    "| pleasant  | 0.75      | 0.94   | 0.84     | 283     |\n",
    "| sad       | 0.49      | 0.59   | 0.53     | 374     |\n",
    "| surprise  | 0.58      | 0.89   | 0.70     | 280     |\n",
    "| **accuracy** |           |        | 0.57     | 3135    |\n",
    "| **macro avg** | 0.59      | 0.60   | 0.57     | 3135    |\n",
    "| **weighted avg** | 0.58      | 0.57   | 0.55     | 3135    |\n",
    "\n",
    "Not much better. Tried better feature extraction for only 2% increase (59%)\n",
    "\n",
    "\n",
    "#### Evaluation v8:\n",
    "\n",
    "Classification Report (Convolutional Neural Network with better features):\n",
    "\n",
    "| Emotion    | Precision | Recall | F1-Score | Support |\n",
    "|------------|-----------|--------|----------|---------|\n",
    "| Angry      | 0.72      | 0.70   | 0.71     | 405     |\n",
    "| Calm       | 0.77      | 0.97   | 0.86     | 291     |\n",
    "| Disgust    | 0.45      | 0.31   | 0.37     | 383     |\n",
    "| Fear       | 0.63      | 0.27   | 0.38     | 388     |\n",
    "| Happy      | 0.46      | 0.43   | 0.44     | 364     |\n",
    "| Neutral    | 0.49      | 0.63   | 0.55     | 367     |\n",
    "| Pleasant   | 0.88      | 0.94   | 0.91     | 283     |\n",
    "| Sad        | 0.51      | 0.63   | 0.57     | 374     |\n",
    "| Surprise   | 0.70      | 0.94   | 0.80     | 280     |\n",
    "| **Accuracy** |           |        | **0.62** | 3135    |\n",
    "| **Macro Avg** | 0.62      | 0.65   | 0.62     | 3135    |\n",
    "| **Weighted Avg** | 0.61      | 0.62   | 0.60     | 3135    |\n",
    "\n",
    "Seeing some progress! This makes sense, since CNN's are better for audio data. Lets try a more advanced CNN Architecture, incorperating Batch Normalization, Dropout, and L2 Regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "9d7b988c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# audio augmentation to help with uneven data\n",
    "def augment_audio(data, sr):\n",
    "    # Time Stretching\n",
    "    try:\n",
    "        stretched_data = librosa.effects.time_stretch(data, rate=1.1)\n",
    "    except Exception as e:\n",
    "        stretched_data = data\n",
    "    \n",
    "    # Shifting \n",
    "    shift = np.random.randint(sr)\n",
    "    shifted_data = np.roll(data, shift)\n",
    "    \n",
    "    # Volume adjustment\n",
    "    amplitude_scale = np.random.uniform(low=0.8, high=1.2)\n",
    "    adjusted_volume_data = data * amplitude_scale\n",
    "    \n",
    "    # Randomly choose one of the augmentation methods to apply\n",
    "    augmentation_methods = [stretched_data, shifted_data, adjusted_volume_data]\n",
    "    \n",
    "    # Check for invalid values in augmentation methods\n",
    "    valid_methods = [method for method in augmentation_methods if len(method) > 0 and not np.isnan(method).any() and not np.isinf(method).any()]\n",
    "    \n",
    "    if valid_methods:\n",
    "        augmented_data = random.choice(valid_methods)\n",
    "    else:\n",
    "        augmented_data = data\n",
    "    \n",
    "    return augmented_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d070eab",
   "metadata": {},
   "source": [
    "## Latest Model: Multiclassification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c371c4f",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ea00e7",
   "metadata": {},
   "source": [
    "\n",
    "### Encode Labels and Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "367be573",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emotion to Encoded Integer Mapping:  {'angry': 0, 'disgust': 1, 'fear': 2, 'happy': 3, 'neutral': 4, 'sad': 5, 'surprise': 6}\n",
      "X_train shape: (9441, 12), y_train shape: (9441, 7)\n",
      "X_test shape: (2361, 12), y_test shape: (2361, 7)\n"
     ]
    }
   ],
   "source": [
    "# Make a copy of the clean_data DataFrame\n",
    "data = clean_data.copy()\n",
    "\n",
    "# Ensure the LabelEncoder is fit on all unique emotion labels\n",
    "le = LabelEncoder()\n",
    "le.fit(data['emotion']) \n",
    "\n",
    "# Encode the emotions for classification\n",
    "data['emotion_encoded'] = le.transform(data['emotion'])  # Store encoded labels in a new column\n",
    "\n",
    "# Create a dictionary to map encoded integers back to the original emotion strings\n",
    "encoded_dict = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "print(\"Emotion to Encoded Integer Mapping: \", encoded_dict)\n",
    "\n",
    "# One-hot encode the labels for neural network output\n",
    "one_hot_encoder = OneHotEncoder(sparse_output=False)  # Ensure sparse_output=False to get a dense array\n",
    "y = one_hot_encoder.fit_transform(data['emotion_encoded'].values.reshape(-1, 1))\n",
    "\n",
    "# Store the labels separately for resampling (if needed)\n",
    "y_labels = data['emotion_encoded']\n",
    "\n",
    "# Drop columns that are not features for model training\n",
    "X = data.drop(['emotion', 'emotion_encoded', 'path', 'filename'], axis=1, errors='ignore')\n",
    "\n",
    "# Split the data into training and test sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test, y_train_labels, y_test_labels = train_test_split(\n",
    "    X, y, y_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Check the shape of the training and test data to confirm everything is correct\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869ef45f",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "c5ac7d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mfccs_mean                       0\n",
      "mfccs_std                        0\n",
      "mfccs_skewness                   0\n",
      "mfccs_kurtosis                   0\n",
      "spectral_centroid_mean           0\n",
      "spectral_centroid_std            0\n",
      "spectral_centroid_skewness       0\n",
      "spectral_centroid_kurtosis       0\n",
      "chroma                           0\n",
      "zero_crossing_rate            9441\n",
      "rms                           9441\n",
      "pitch                            0\n",
      "dtype: int64\n",
      "mfccs_mean                       0\n",
      "mfccs_std                        0\n",
      "mfccs_skewness                   0\n",
      "mfccs_kurtosis                   0\n",
      "spectral_centroid_mean           0\n",
      "spectral_centroid_std            0\n",
      "spectral_centroid_skewness       0\n",
      "spectral_centroid_kurtosis       0\n",
      "chroma                           0\n",
      "zero_crossing_rate            2361\n",
      "rms                           2361\n",
      "pitch                            0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Function to flatten nested lists or arrays into a single list\n",
    "def flatten_nested_list(value):\n",
    "    if isinstance(value, (list, np.ndarray)):\n",
    "        return [item for sublist in value for item in (sublist if isinstance(sublist, (list, np.ndarray)) else [sublist])]\n",
    "    else:\n",
    "        return [value]\n",
    "\n",
    "# Columns that may contain nested lists\n",
    "columns_to_flatten = ['mfccs_mean', 'mfccs_std', 'mfccs_skewness', 'mfccs_kurtosis', \n",
    "                      'chroma', 'zero_crossing_rate', 'rms', 'pitch']\n",
    "\n",
    "# Flatten and convert lists of length 1 to scalars in the training set\n",
    "for col in columns_to_flatten:\n",
    "    X_train[col] = X_train[col].apply(flatten_nested_list)\n",
    "    X_train[col] = X_train[col].apply(lambda x: x[0] if isinstance(x, list) and len(x) == 1 else x)\n",
    "\n",
    "# Ensure all columns in the training set are of type float\n",
    "X_train = X_train.astype(float)\n",
    "\n",
    "# Flatten and convert lists of length 1 to scalars in the test set\n",
    "for col in columns_to_flatten:\n",
    "    X_test[col] = X_test[col].apply(flatten_nested_list)\n",
    "    X_test[col] = X_test[col].apply(lambda x: x[0] if isinstance(x, list) and len(x) == 1 else x)\n",
    "\n",
    "# Ensure all columns in the test set are of type float\n",
    "X_test = X_test.astype(float)\n",
    "\n",
    "# Check for missing values\n",
    "print(X_train.isnull().sum())\n",
    "print(X_test.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e9f9b4",
   "metadata": {},
   "source": [
    "### Reshape Data for CNN Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "f4fb90d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped X_train_aug_cnn shape: (13476, 76, 1)\n",
      "Reshaped X_test_cnn shape: (2361, 12, 1)\n"
     ]
    }
   ],
   "source": [
    "# Reshape the training data for CNN (samples, timesteps, features)\n",
    "X_train_cnn = X_train.values.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test_cnn = X_test.values.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "print(f\"Reshaped X_train_aug_cnn shape: {X_train_aug_cnn.shape}\")\n",
    "print(f\"Reshaped X_test_cnn shape: {X_test_cnn.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fe64d0",
   "metadata": {},
   "source": [
    "### Define CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "4bab95be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_21 (Conv1D)          (None, 10, 32)            128       \n",
      "                                                                 \n",
      " max_pooling1d_16 (MaxPooli  (None, 5, 32)             0         \n",
      " ng1D)                                                           \n",
      "                                                                 \n",
      " conv1d_22 (Conv1D)          (None, 3, 64)             6208      \n",
      "                                                                 \n",
      " max_pooling1d_17 (MaxPooli  (None, 1, 64)             0         \n",
      " ng1D)                                                           \n",
      "                                                                 \n",
      " conv1d_23 (Conv1D)          (None, 1, 128)            8320      \n",
      "                                                                 \n",
      " flatten_5 (Flatten)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_38 (Dense)            (None, 256)               33024     \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_39 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_40 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_41 (Dense)            (None, 6)                 390       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 89222 (348.52 KB)\n",
      "Trainable params: 89222 (348.52 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the CNN model\n",
    "model = Sequential()\n",
    "\n",
    "# First convolutional layer with 32 filters, kernel size of 3, and relu activation\n",
    "model.add(Conv1D(32, kernel_size=3, activation='relu', input_shape=(X_train_cnn.shape[1], 1)))\n",
    "model.add(MaxPooling1D(pool_size=2))  # Max pooling to reduce dimensionality\n",
    "\n",
    "# Second convolutional layer\n",
    "model.add(Conv1D(64, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "# Third convolutional layer\n",
    "# Use kernel_size=1 since the input size has become very small (1 in time dimension)\n",
    "model.add(Conv1D(128, kernel_size=1, activation='relu'))\n",
    "\n",
    "# No further pooling here as the dimension is already small enough\n",
    "\n",
    "# Flatten the output to feed into Dense layers\n",
    "model.add(Flatten())\n",
    "\n",
    "# Fully connected layers\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.4))  # Dropout to prevent overfitting\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "\n",
    "# Output layer (number of emotions, use softmax for multiclass classification)\n",
    "model.add(Dense(y_train.shape[1], activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Summary of the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ea60a6",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "8d2db98e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_cnn shape: (9441, 12, 1)\n",
      "y_train shape: (9441, 6)\n",
      "Epoch 1/50\n",
      "236/236 [==============================] - 1s 2ms/step - loss: 4.3298 - accuracy: 0.1709 - val_loss: 1.7889 - val_accuracy: 0.2028\n",
      "Epoch 2/50\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 1.7940 - accuracy: 0.1875 - val_loss: 1.7878 - val_accuracy: 0.2043\n",
      "Epoch 3/50\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 1.7879 - accuracy: 0.1927 - val_loss: 1.7850 - val_accuracy: 0.2102\n",
      "Epoch 4/50\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 1.7905 - accuracy: 0.1907 - val_loss: 1.7864 - val_accuracy: 0.2054\n",
      "Epoch 5/50\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 1.7890 - accuracy: 0.1920 - val_loss: 1.7875 - val_accuracy: 0.2028\n",
      "Epoch 6/50\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 1.7892 - accuracy: 0.1925 - val_loss: 1.7877 - val_accuracy: 0.2028\n",
      "Epoch 7/50\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 1.7891 - accuracy: 0.1898 - val_loss: 1.7876 - val_accuracy: 0.2028\n",
      "Epoch 8/50\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 1.7899 - accuracy: 0.1895 - val_loss: 1.7877 - val_accuracy: 0.2028\n"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Check the shapes of X_train_cnn and y_train to ensure they match\n",
    "print(f\"X_train_cnn shape: {X_train_cnn.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "\n",
    "# Make sure the number of samples is the same\n",
    "assert X_train_cnn.shape[0] == y_train.shape[0], \"Mismatch between X and y samples.\"\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_cnn, y_train, validation_split=0.2, epochs=50, batch_size=32, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c44e8de",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "879cb3ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.7871, Test Accuracy: 0.1944\n",
      "74/74 [==============================] - 0s 717us/step\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       angry       0.00      0.00      0.00       362\n",
      "     disgust       0.00      0.00      0.00       385\n",
      "        fear       0.00      0.00      0.00       430\n",
      "       happy       0.19      1.00      0.32       448\n",
      "     neutral       0.00      0.00      0.00       328\n",
      "         sad       0.48      0.03      0.05       408\n",
      "\n",
      "    accuracy                           0.19      2361\n",
      "   macro avg       0.11      0.17      0.06      2361\n",
      "weighted avg       0.12      0.19      0.07      2361\n",
      "\n",
      "Confusion Matrix:\n",
      " [[  0   0   0 362   0   0]\n",
      " [  0   0   0 382   0   3]\n",
      " [  0   0   0 423   0   7]\n",
      " [  0   0   0 448   0   0]\n",
      " [  0   0   0 326   0   2]\n",
      " [  0   0   0 397   0  11]]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test_cnn, y_test, verbose=0)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_proba = model.predict(X_test_cnn)\n",
    "y_pred = np.argmax(y_pred_proba, axis=1)  # Get the predicted labels\n",
    "y_test_classes = np.argmax(y_test, axis=1)  # Get the true labels\n",
    "\n",
    "# Print evaluation metrics using the zero_division parameter to handle undefined metrics\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test_classes, y_pred, target_names=le.classes_, zero_division=0))  # Set zero_division=0 to handle undefined metrics\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test_classes, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34aa2676",
   "metadata": {},
   "source": [
    "## Binary Classification\n",
    "\n",
    "In the most recent report, the model's performance isn't satisfactory for reliably predicting specific emotions. The weighted average precision of 0.55 and F-score of 0.54 indicate that the model struggles to accurately classify the different categories. Having a relatively small dataset of 11,802 records can limit the model's ability to learn and generalize the complex patterns  of emotional recognition in voice. \n",
    "\n",
    "Simplifying the problem into positive/negative sentiments may improve metrics. With reduced complexity, the model could better grasp underlying patterns. Larger class sizes could mitigate data scarcity issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4836b183",
   "metadata": {},
   "source": [
    "I'm going to consider 'neutral' as positive,\n",
    "\n",
    "    1. because we need more 'positive' emotion\n",
    "    2. because in our business problem, 'neutral' is a good thing!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3a3bca",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "1baa2c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary_emotion\n",
      "0    7764\n",
      "1    4038\n",
      "Name: count, dtype: int64\n",
      "X_train shape: (9441, 12), X_test shape: (2361, 12)\n",
      "y_train shape: (9441,), y_test shape: (2361,)\n"
     ]
    }
   ],
   "source": [
    "# Define positive and negative emotions\n",
    "positive_emotions = ['happy', 'pleasant', 'surprise', 'calm', 'neutral']\n",
    "negative_emotions = ['sad', 'angry', 'disgust', 'fear']\n",
    "\n",
    "# Copy the clean data and create a binary emotion column\n",
    "bin_data = clean_data.copy()\n",
    "\n",
    "# Create the binary emotion column, set 1 for positive emotions, 0 for negative emotions\n",
    "bin_data['binary_emotion'] = bin_data['emotion'].apply(\n",
    "    lambda x: 1 if x in positive_emotions else 0 if x in negative_emotions else None\n",
    ")\n",
    "\n",
    "# Drop rows where binary_emotion is None (in case there are emotions we don't care about)\n",
    "bin_data.dropna(subset=['binary_emotion'], inplace=True)\n",
    "\n",
    "# Convert the binary_emotion column to integer type\n",
    "bin_data['binary_emotion'] = bin_data['binary_emotion'].astype(int)\n",
    "\n",
    "# Check the distribution of binary emotions (for sanity checking)\n",
    "print(bin_data['binary_emotion'].value_counts())\n",
    "\n",
    "# Drop unnecessary columns like 'emotion', 'path', and 'filename'\n",
    "X = bin_data.drop(['binary_emotion', 'emotion', 'path', 'filename'], axis=1, errors='ignore')\n",
    "y = bin_data['binary_emotion']\n",
    "\n",
    "# Split the dataset into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Check the shape of the split datasets\n",
    "print(f\"X_train shape: {X_train.shape}, X_test shape: {X_test.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}, y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b928c0",
   "metadata": {},
   "source": [
    "### Data Augmentation for Positive Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c440ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "af5082ef",
   "metadata": {},
   "source": [
    "### Flatten Nested Lists and Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "8f4dd2e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mfccs_mean                    0\n",
      "mfccs_std                     0\n",
      "mfccs_skewness                0\n",
      "mfccs_kurtosis                0\n",
      "spectral_centroid_mean        0\n",
      "spectral_centroid_std         0\n",
      "spectral_centroid_skewness    0\n",
      "spectral_centroid_kurtosis    0\n",
      "chroma                        0\n",
      "zero_crossing_rate            0\n",
      "rms                           0\n",
      "pitch                         0\n",
      "dtype: int64\n",
      "mfccs_mean                    0\n",
      "mfccs_std                     0\n",
      "mfccs_skewness                0\n",
      "mfccs_kurtosis                0\n",
      "spectral_centroid_mean        0\n",
      "spectral_centroid_std         0\n",
      "spectral_centroid_skewness    0\n",
      "spectral_centroid_kurtosis    0\n",
      "chroma                        0\n",
      "zero_crossing_rate            0\n",
      "rms                           0\n",
      "pitch                         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for NaNs in the dataset\n",
    "print(X_test.isna().sum())\n",
    "print(X_train.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "f7503af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a utility function to flatten nested lists or arrays\n",
    "def flatten_nested_list(value):\n",
    "    if isinstance(value, (list, np.ndarray)):\n",
    "        return [item for sublist in value for item in (sublist if isinstance(sublist, (list, np.ndarray)) else [sublist])]\n",
    "    else:\n",
    "        return [value]\n",
    "\n",
    "# List of columns that need flattening\n",
    "columns_to_flatten = ['mfccs_mean', 'mfccs_std', 'mfccs_skewness', 'mfccs_kurtosis',\n",
    "                      'chroma', 'zero_crossing_rate', 'rms', 'pitch']\n",
    "\n",
    "# Flatten the columns in the training set\n",
    "for col in columns_to_flatten:\n",
    "    X_train_aug[col] = X_train_aug[col].apply(flatten_nested_list)\n",
    "    X_train_aug[col] = X_train_aug[col].apply(lambda x: x[0] if isinstance(x, list) and len(x) == 1 else x)\n",
    "\n",
    "# Ensure all columns are of type float\n",
    "X_train_aug = X_train_aug.astype(float)\n",
    "\n",
    "# Repeat the process for the test set\n",
    "for col in columns_to_flatten:\n",
    "    X_test[col] = X_test[col].apply(flatten_nested_list)\n",
    "    X_test[col] = X_test[col].apply(lambda x: x[0] if isinstance(x, list) and len(x) == 1 else x)\n",
    "\n",
    "# Ensure all columns in the test set are of type float\n",
    "X_test = X_test.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2be9ec",
   "metadata": {},
   "source": [
    "### Reshape Data for CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "9c5d32cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape X_train and X_test to be 3D for Conv1D\n",
    "X_train_cnn = X_train.values.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test_cnn = X_test.values.reshape((X_test.shape[0], X_test.shape[1], 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046a09e7",
   "metadata": {},
   "source": [
    "### Define and Train the CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "582a269c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_27 (Conv1D)          (None, 10, 32)            128       \n",
      "                                                                 \n",
      " max_pooling1d_20 (MaxPooli  (None, 5, 32)             0         \n",
      " ng1D)                                                           \n",
      "                                                                 \n",
      " conv1d_28 (Conv1D)          (None, 3, 64)             6208      \n",
      "                                                                 \n",
      " max_pooling1d_21 (MaxPooli  (None, 1, 64)             0         \n",
      " ng1D)                                                           \n",
      "                                                                 \n",
      " conv1d_29 (Conv1D)          (None, 1, 128)            8320      \n",
      "                                                                 \n",
      " flatten_7 (Flatten)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_46 (Dense)            (None, 256)               33024     \n",
      "                                                                 \n",
      " dropout_15 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_47 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_48 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_49 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 88897 (347.25 KB)\n",
      "Trainable params: 88897 (347.25 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "X_train_cnn shape: (9441, 12, 1)\n",
      "y_train shape: (9441,)\n",
      "Epoch 1/50\n",
      "236/236 [==============================] - 2s 2ms/step - loss: 1.9503 - accuracy: 0.5734 - val_loss: 0.6700 - val_accuracy: 0.6019\n",
      "Epoch 2/50\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.6937 - accuracy: 0.6188 - val_loss: 0.6496 - val_accuracy: 0.6321\n",
      "Epoch 3/50\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.6781 - accuracy: 0.6357 - val_loss: 0.6494 - val_accuracy: 0.6474\n",
      "Epoch 4/50\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.6587 - accuracy: 0.6441 - val_loss: 0.6510 - val_accuracy: 0.6474\n",
      "Epoch 5/50\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.6635 - accuracy: 0.6423 - val_loss: 0.6477 - val_accuracy: 0.6474\n",
      "Epoch 6/50\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.6519 - accuracy: 0.6498 - val_loss: 0.6528 - val_accuracy: 0.6474\n",
      "Epoch 7/50\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.6517 - accuracy: 0.6471 - val_loss: 0.6456 - val_accuracy: 0.6474\n",
      "Epoch 8/50\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.6462 - accuracy: 0.6540 - val_loss: 0.6472 - val_accuracy: 0.6490\n",
      "Epoch 9/50\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.6474 - accuracy: 0.6499 - val_loss: 0.6505 - val_accuracy: 0.6474\n",
      "Epoch 10/50\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.6460 - accuracy: 0.6537 - val_loss: 0.6544 - val_accuracy: 0.6353\n",
      "Epoch 11/50\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.6429 - accuracy: 0.6514 - val_loss: 0.6436 - val_accuracy: 0.6480\n",
      "Epoch 12/50\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.6426 - accuracy: 0.6532 - val_loss: 0.6446 - val_accuracy: 0.6474\n",
      "Epoch 13/50\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.6416 - accuracy: 0.6531 - val_loss: 0.6526 - val_accuracy: 0.6474\n",
      "Epoch 14/50\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.6430 - accuracy: 0.6544 - val_loss: 0.6468 - val_accuracy: 0.6474\n",
      "Epoch 15/50\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.6442 - accuracy: 0.6533 - val_loss: 0.6463 - val_accuracy: 0.6474\n",
      "Epoch 16/50\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.6402 - accuracy: 0.6545 - val_loss: 0.6408 - val_accuracy: 0.6474\n",
      "Epoch 17/50\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.6442 - accuracy: 0.6559 - val_loss: 0.6464 - val_accuracy: 0.6474\n",
      "Epoch 18/50\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.6412 - accuracy: 0.6536 - val_loss: 0.6458 - val_accuracy: 0.6474\n",
      "Epoch 19/50\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.6415 - accuracy: 0.6514 - val_loss: 0.6465 - val_accuracy: 0.6474\n",
      "Epoch 20/50\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.6415 - accuracy: 0.6532 - val_loss: 0.6471 - val_accuracy: 0.6474\n",
      "Epoch 21/50\n",
      "236/236 [==============================] - 0s 2ms/step - loss: 0.6449 - accuracy: 0.6552 - val_loss: 0.6437 - val_accuracy: 0.6474\n",
      "Test Loss: 0.6265, Test Accuracy: 0.6713\n"
     ]
    }
   ],
   "source": [
    "# Define the CNN model for binary classification\n",
    "model = Sequential()\n",
    "\n",
    "# First convolutional layer\n",
    "model.add(Conv1D(32, kernel_size=3, activation='relu', input_shape=(X_train_cnn.shape[1], 1)))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "# Second convolutional layer\n",
    "model.add(Conv1D(64, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "# Third convolutional layer (small kernel since input size is small)\n",
    "model.add(Conv1D(128, kernel_size=1, activation='relu'))\n",
    "\n",
    "# Flatten the output for fully connected layers\n",
    "model.add(Flatten())\n",
    "\n",
    "# Fully connected layers\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "\n",
    "# Output layer for binary classification (use sigmoid for binary classification)\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()\n",
    "\n",
    "# Define early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Check the shapes of X_train and y_train to ensure they match\n",
    "print(f\"X_train_cnn shape: {X_train_cnn.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "\n",
    "# Ensure there is no mismatch between X_train and y_train\n",
    "assert X_train_cnn.shape[0] == y_train.shape[0], \"Mismatch between X and y samples.\"\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_cnn, y_train, validation_split=0.2, epochs=50, batch_size=32, callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test_cnn, y_test, verbose=0)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84a34be",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "a8627025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74/74 [==============================] - 0s 730us/step\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      1.00      0.80      1585\n",
      "           1       0.00      0.00      0.00       776\n",
      "\n",
      "    accuracy                           0.67      2361\n",
      "   macro avg       0.34      0.50      0.40      2361\n",
      "weighted avg       0.45      0.67      0.54      2361\n",
      "\n",
      "Confusion Matrix:\n",
      " [[1585    0]\n",
      " [ 776    0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\Anaconda\\envs\\learn-env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "H:\\Anaconda\\envs\\learn-env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "H:\\Anaconda\\envs\\learn-env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Predict probabilities and convert to binary predictions\n",
    "y_pred_proba = model.predict(X_test_cnn)\n",
    "y_pred = (y_pred_proba > 0.5).astype(int).flatten()\n",
    "\n",
    "# Print classification metrics\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b780f44",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Let's re-approach, and build a neural network model (which should me more appropriate for audio data!)\n",
    "\n",
    "##"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
