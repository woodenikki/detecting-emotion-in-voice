{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b779dda",
   "metadata": {},
   "source": [
    "# MoodWave: Voice-Driven Emotion Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbef6b6",
   "metadata": {},
   "source": [
    "## Business Understanding\n",
    "\n",
    "When working as a customer service agent, you are bound to run into the whole spectrum of emotions - annoyance, gratefulness, anger, or complete neutrality. These agents are often monitored, scored on how well - and how many issues they address. \n",
    "\n",
    "A satisfied customer - either leaving a positive review, or having the call recorded and review - will leave a good impression on the agent. \n",
    "A frustrated customer is more likely to leave a negative review, or a manager reviews a call that went poorly - it reflects negatively on the agent. \n",
    "\n",
    "Implementing AI into the customer support pipeline can be beneficial in many ways:\n",
    "\n",
    "- If an agent knows as soon as their customer is upset, they can adjust how they speak to them, or you can even adapt the script the agent uses as the customerâ€™s emotions fluctuate. \n",
    "- The agent feels more secure in their responses, or know that they can escalate to someone better equipped to handle difficult customers. \n",
    "- The customer feels heard, finds a resolution faster, and has an overall better experience!\n",
    "\n",
    "#### Objective: Create a model to identify customer emotion (Upset/Not Upset) over the phone."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fadc2d",
   "metadata": {},
   "source": [
    "## Data Understanding\n",
    "\n",
    "In this project, we are dealing with four datasets containing English audio recordings in the .wav format. Each audio recording is labeled with an emotion that the speaker is evoking in their statement. Our goal is to build a model that can successfully map an emotion to a given voice clip of someone speaking.\n",
    "\n",
    "> Compiled datsets can be found on [Kaggle](https://www.kaggle.com/datasets/dmitrybabko/speech-emotion-recognition-en).\n",
    "\n",
    "To achieve this, we will extract various features from the audio recordings that are relevant for analyzing speech and emotion. Here are the features we will be working with and their significance for working with audio:\n",
    "\n",
    "1. **Mel-frequency Cepstral Coefficients (MFCCs):** MFCCs are a compact representation of the short-term power spectrum of a sound. They are widely used in speech recognition and audio analysis tasks because they capture the essential characteristics of the audio signal while being robust to noise and other variabilities. MFCCs are particularly useful for identifying the phonetic content of speech, which can be helpful in determining the emotional state of the speaker.\n",
    "\n",
    "2. **Spectral Centroid:** The spectral centroid is a measure of the brightness or sharpness of a sound. It represents the weighted mean frequency of the spectrum and can be used to distinguish between different types of sounds or emotions. For example, a bright, harsh sound might have a higher spectral centroid than a mellow, soft sound.\n",
    "\n",
    "3. **Chroma Features:** Chroma features describe the distribution of energy across different pitch classes (notes) in the audio signal. They are useful for capturing tonal information, which can be relevant for identifying emotions in speech, particularly those related to intonation patterns and stress.\n",
    "\n",
    "4. **Zero-Crossing Rate:** The zero-crossing rate is a measure of the number of times the audio signal crosses the zero amplitude axis within a given time frame. It can be used to distinguish between different types of sounds, such as voiced and unvoiced speech, and can provide insights into the energy distribution of the audio signal.\n",
    "RMS Energy: The Root Mean Square (RMS) energy is a measure of the overall energy or loudness of an audio signal. It can be useful for detecting variations in volume or intensity, which can be indicative of certain emotions, such as anger or excitement.\n",
    "\n",
    "---\n",
    "\n",
    "By extracting and analyzing these features, we can capture various acoustic characteristics of the speech signal that may be relevant for distinguishing between different emotions. This multi-faceted approach can provide a more comprehensive representation of the audio data, potentially leading to better performance in the emotion classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d259fca",
   "metadata": {},
   "source": [
    "## Data Preperation\n",
    "\n",
    "First, we will combine our 4 datasets in English: Crema, Ravdess, Savee and Tess. Each of them contains audio in .wav format with some main labels. (Note - not all datasets represent the same emotions, we will clean up the data labels to be as generic / inclusive as possible.\n",
    "\n",
    "We will pull each dataset into their own dataframe, making note of *where* the file is, so we can later pull our features from each audio file. \n",
    "\n",
    "Then, we will merge them all into one dataframe and extract our audio features as mentioned earlier:\n",
    "\n",
    "- Mel-frequency cepstral coefficients (MFCCs)\n",
    "- Spectral centroid\n",
    "- Chroma features\n",
    "- Zero-crossing rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38d5029f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import zipfile\n",
    "import librosa\n",
    "import random\n",
    "import os\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures, LabelEncoder, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import resample\n",
    "from sklearn.utils import class_weight\n",
    "from scipy.stats import skew, kurtosis\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abe952cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data is zipped, and stored in folders for which dataset they came from:\n",
    "\n",
    "# Define the path to the zipped dataset\n",
    "zip_file_path = 'dataset.zip'\n",
    "extracted_folder_path = 'dataset'\n",
    "\n",
    "# Unzip the dataset\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extracted_folder_path)\n",
    "\n",
    "# Crema\n",
    "# Ravdess\n",
    "# Savee\n",
    "# Tess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb9aa452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              filename  emotion           path\n",
      "0  1001_DFA_ANG_XX.wav    angry  dataset\\Crema\n",
      "1  1001_DFA_DIS_XX.wav  disgust  dataset\\Crema\n",
      "2  1001_DFA_FEA_XX.wav     fear  dataset\\Crema\n",
      "3  1001_DFA_HAP_XX.wav    happy  dataset\\Crema\n",
      "4  1001_DFA_NEU_XX.wav  neutral  dataset\\Crema\n"
     ]
    }
   ],
   "source": [
    "# Define the path to the Crema folder\n",
    "crema_folder_path = os.path.join(extracted_folder_path, 'Crema')\n",
    "\n",
    "# Verify that we can access the files and extract emotion labels\n",
    "data = []\n",
    "\n",
    "# Loop through each file in the Crema folder\n",
    "for file_name in os.listdir(crema_folder_path):\n",
    "    if file_name.endswith('.wav'):\n",
    "        # Extract the emotion label from the filename\n",
    "        parts = file_name.split('_')\n",
    "        emotion_code = parts[2]\n",
    "        \n",
    "        # Map the emotion code to the actual emotion label\n",
    "        emotion_map = {\n",
    "            'SAD': 'sadness',\n",
    "            'ANG': 'angry',\n",
    "            'DIS': 'disgust',\n",
    "            'FEA': 'fear',\n",
    "            'HAP': 'happy',\n",
    "            'NEU': 'neutral'\n",
    "        }\n",
    "        emotion_label = emotion_map.get(emotion_code, 'unknown')\n",
    "        \n",
    "        # Store the data with the directory path minus the filename\n",
    "        data.append({'filename': file_name, 'emotion': emotion_label, 'path': crema_folder_path})\n",
    "\n",
    "# Convert the data into a DataFrame for easy access\n",
    "df_crema = pd.DataFrame(data)\n",
    "\n",
    "# Display the first few rows to verify\n",
    "print(df_crema.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0470988c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             filename emotion                    path\n",
      "0  OAF_back_angry.wav   angry  dataset\\Tess\\OAF_angry\n",
      "1   OAF_bar_angry.wav   angry  dataset\\Tess\\OAF_angry\n",
      "2  OAF_base_angry.wav   angry  dataset\\Tess\\OAF_angry\n",
      "3  OAF_bath_angry.wav   angry  dataset\\Tess\\OAF_angry\n",
      "4  OAF_bean_angry.wav   angry  dataset\\Tess\\OAF_angry\n"
     ]
    }
   ],
   "source": [
    "# Define the path to the Tess folder\n",
    "tess_folder_path = os.path.join(extracted_folder_path, 'Tess')\n",
    "\n",
    "# Prepare to store the data\n",
    "data = []\n",
    "\n",
    "# Loop through each emotion folder in the Tess directory\n",
    "for emotion_folder in os.listdir(tess_folder_path):\n",
    "    # Get the full path to the emotion folder\n",
    "    emotion_folder_path = os.path.join(tess_folder_path, emotion_folder)\n",
    "    \n",
    "    # Extract the emotion from the folder name (e.g., \"OAF_angry\" -> \"angry\")\n",
    "    emotion_label = emotion_folder.split('_')[1]\n",
    "    \n",
    "    # Loop through each file in the emotion folder\n",
    "    for file_name in os.listdir(emotion_folder_path):\n",
    "        if file_name.endswith('.wav'):\n",
    "            # Store the data with the directory path minus the filename\n",
    "            data.append({\n",
    "                'filename': file_name, \n",
    "                'emotion': emotion_label, \n",
    "                'path': emotion_folder_path\n",
    "            })\n",
    "\n",
    "# Convert the data into a DataFrame for easy access\n",
    "df_tess = pd.DataFrame(data)\n",
    "\n",
    "# Display the first few rows to verify\n",
    "print(df_tess.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9662b2c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     filename  emotion           path\n",
      "0  DC_a01.wav  unknown  dataset\\Savee\n",
      "1  DC_a02.wav  unknown  dataset\\Savee\n",
      "2  DC_a03.wav  unknown  dataset\\Savee\n",
      "3  DC_a04.wav  unknown  dataset\\Savee\n",
      "4  DC_a05.wav  unknown  dataset\\Savee\n"
     ]
    }
   ],
   "source": [
    "# Define the path to the Savee folder\n",
    "savee_folder_path = os.path.join(extracted_folder_path, 'Savee')\n",
    "\n",
    "# Prepare to store the data\n",
    "data = []\n",
    "\n",
    "# Define the emotion mapping based on the prefixes\n",
    "emotion_map = {\n",
    "    'a': 'anger',\n",
    "    'd': 'disgust',\n",
    "    'f': 'fear',\n",
    "    'h': 'happiness',\n",
    "    'n': 'neutral',\n",
    "    'sa': 'sadness',\n",
    "    'su': 'surprise'\n",
    "}\n",
    "\n",
    "# Loop through each file in the Savee folder\n",
    "for file_name in os.listdir(savee_folder_path):\n",
    "    if file_name.endswith('.wav'):\n",
    "        # Extract the prefix from the filename to determine the emotion\n",
    "        prefix = file_name.split('_')[1][:2]\n",
    "        \n",
    "        # Map the prefix to the corresponding emotion\n",
    "        emotion_label = emotion_map.get(prefix, 'unknown')\n",
    "        \n",
    "        # Store the data with the directory path minus the filename\n",
    "        data.append({\n",
    "            'filename': file_name, \n",
    "            'emotion': emotion_label, \n",
    "            'path': savee_folder_path\n",
    "        })\n",
    "\n",
    "# Convert the data into a DataFrame for easy access\n",
    "df_savee = pd.DataFrame(data)\n",
    "\n",
    "# Display the first few rows to verify\n",
    "print(df_savee.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd168774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   filename  emotion  \\\n",
      "0  03-01-01-01-01-01-01.wav  neutral   \n",
      "1  03-01-01-01-01-02-01.wav  neutral   \n",
      "2  03-01-01-01-02-01-01.wav  neutral   \n",
      "3  03-01-01-01-02-02-01.wav  neutral   \n",
      "4  03-01-02-01-01-01-01.wav     calm   \n",
      "\n",
      "                                                path  \n",
      "0  dataset\\Ravdess\\audio_speech_actors_01-24\\Acto...  \n",
      "1  dataset\\Ravdess\\audio_speech_actors_01-24\\Acto...  \n",
      "2  dataset\\Ravdess\\audio_speech_actors_01-24\\Acto...  \n",
      "3  dataset\\Ravdess\\audio_speech_actors_01-24\\Acto...  \n",
      "4  dataset\\Ravdess\\audio_speech_actors_01-24\\Acto...  \n"
     ]
    }
   ],
   "source": [
    "# Define the path to the Ravdess folder\n",
    "ravdess_folder_path = os.path.join(extracted_folder_path, 'Ravdess', 'audio_speech_actors_01-24')\n",
    "\n",
    "# Prepare to store the data\n",
    "data = []\n",
    "\n",
    "# Define the emotion mapping based on the third component in the filename\n",
    "emotion_map = {\n",
    "    '01': 'neutral',\n",
    "    '02': 'calm',\n",
    "    '03': 'happy',\n",
    "    '04': 'sad',\n",
    "    '05': 'angry',\n",
    "    '06': 'fearful',\n",
    "    '07': 'disgust',\n",
    "    '08': 'surprised'\n",
    "}\n",
    "\n",
    "# Loop through each actor's folder in the Ravdess directory\n",
    "for actor_folder in os.listdir(ravdess_folder_path):\n",
    "    actor_folder_path = os.path.join(ravdess_folder_path, actor_folder)\n",
    "    \n",
    "    # Loop through each file in the actor's folder\n",
    "    for file_name in os.listdir(actor_folder_path):\n",
    "        if file_name.endswith('.wav'):\n",
    "            # Extract the third component from the filename to determine the emotion\n",
    "            emotion_code = file_name.split('-')[2]\n",
    "            \n",
    "            # Map the emotion code to the corresponding emotion label\n",
    "            emotion_label = emotion_map.get(emotion_code, 'unknown')\n",
    "            \n",
    "            # Store the data with the directory path minus the filename\n",
    "            data.append({\n",
    "                'filename': file_name, \n",
    "                'emotion': emotion_label, \n",
    "                'path': actor_folder_path\n",
    "            })\n",
    "\n",
    "# Convert the data into a DataFrame for easy access\n",
    "df_ravdess = pd.DataFrame(data)\n",
    "\n",
    "# Display the first few rows to verify\n",
    "print(df_ravdess.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb11e7c",
   "metadata": {},
   "source": [
    "### Combining datasets \n",
    "\n",
    "We will merge the datsets into one dataframe, and assign unique identifiers\n",
    "- Concatenate the DataFrames for each dataset.\n",
    "- Assign a unique ID to each entry based on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5ec5d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       id             filename  emotion           path\n",
      "0  c_0001  1001_DFA_ANG_XX.wav    angry  dataset\\Crema\n",
      "1  c_0002  1001_DFA_DIS_XX.wav  disgust  dataset\\Crema\n",
      "2  c_0003  1001_DFA_FEA_XX.wav     fear  dataset\\Crema\n",
      "3  c_0004  1001_DFA_HAP_XX.wav    happy  dataset\\Crema\n",
      "4  c_0005  1001_DFA_NEU_XX.wav  neutral  dataset\\Crema\n"
     ]
    }
   ],
   "source": [
    "# Add a unique ID column to each dataset\n",
    "df_crema['id'] = ['c_{:04d}'.format(i + 1) for i in range(len(df_crema))]\n",
    "df_tess['id'] = ['t_{:04d}'.format(i + 1) for i in range(len(df_tess))]\n",
    "df_savee['id'] = ['s_{:04d}'.format(i + 1) for i in range(len(df_savee))]\n",
    "df_ravdess['id'] = ['r_{:04d}'.format(i + 1) for i in range(len(df_ravdess))]\n",
    "\n",
    "# Merge the datasets into a single DataFrame\n",
    "merged_data = pd.concat([df_crema, df_tess, df_savee, df_ravdess], ignore_index=True)\n",
    "\n",
    "# Reorder columns to have 'id' as the first column\n",
    "merged_data = merged_data[['id', 'filename', 'emotion', 'path']]\n",
    "\n",
    "# Display the first few rows of the combined DataFrame\n",
    "print(merged_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05b8dbcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows in dataset: 12162\n"
     ]
    }
   ],
   "source": [
    "# remember, we need at least 1000 rows to meet our requirements. \n",
    "print(f\"Total rows in dataset: {merged_data.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f387e934",
   "metadata": {},
   "source": [
    "### Extracting Features\n",
    "\n",
    "Again, these are the features we will extract:\n",
    "\n",
    "- **Mel-frequency cepstral coefficients (MFCCs):** Represents the short-term power spectrum of sound, commonly used in speech and audio processing to capture the timbral texture of audio.\n",
    "- **Spectral centroid:** Indicates the \"center of mass\" of the spectrum and is often associated with the perceived brightness of a sound.\n",
    "- **Chroma features:** Represents the 12 different pitch classes and captures harmonic and melodic characteristics of music / voice.\n",
    "- **Zero-crossing rate:** Measures the rate at which the signal changes sign, giving insight into the noisiness or percussiveness of the sound.\n",
    "- **RMS energy:** Reflects the root mean square of the audio signal and indicates the energy or loudness of the sound.\n",
    "- **Pitch:** Refers to the perceived frequency of a sound, determining how high or low a sound is.\n",
    "\n",
    "We will be using the `librosa` package to process these audio features. [Here](https://librosa.org/doc/latest/index.html) is a link to the librosa documentation.\n",
    "\n",
    "**Note**: adding suppression for *UserWarning: Trying to estimate tuning from empty frequency set*. This is likely do to either:* **silence / low energy** (too quiet to perform reliable pitch estimation), or the file had **too short of a duration**. This warning shows up even when setting the pitch to 0 in this case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b93de75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_time_series(file_path, sr=22050, n_mfcc=13, hop_length=512, n_fft=2048, duration=None, max_pad_len=862):\n",
    "    try:\n",
    "        # Load the audio file\n",
    "        y, sr = librosa.load(file_path, sr=sr, duration=duration)\n",
    "\n",
    "        # Ensure consistent audio length by padding or truncating\n",
    "        if duration is not None:\n",
    "            max_samples = int(sr * duration)\n",
    "            if len(y) < max_samples:\n",
    "                y = np.pad(y, (0, max_samples - len(y)), mode='constant')\n",
    "            else:\n",
    "                y = y[:max_samples]\n",
    "\n",
    "        # Extract features\n",
    "        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc, hop_length=hop_length, n_fft=n_fft)\n",
    "        chroma = librosa.feature.chroma_stft(y=y, sr=sr, hop_length=hop_length, n_fft=n_fft)\n",
    "        spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr, hop_length=hop_length, n_fft=n_fft)\n",
    "        zero_crossing_rate = librosa.feature.zero_crossing_rate(y, hop_length=hop_length)\n",
    "\n",
    "        # Stack features along the feature axis\n",
    "        features = np.vstack([mfccs, chroma, spectral_centroid, zero_crossing_rate])\n",
    "\n",
    "        # Transpose to get shape (time_steps, features)\n",
    "        features = features.T\n",
    "\n",
    "        # Pad to ensure consistent time dimension (if needed)\n",
    "        if features.shape[0] < max_pad_len:\n",
    "            pad_width = max_pad_len - features.shape[0]\n",
    "            features = np.pad(features, ((0, pad_width), (0, 0)), mode='constant')\n",
    "        else:\n",
    "            features = features[:max_pad_len, :]\n",
    "\n",
    "        return features\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "        return None  # Return None explicitly if there's an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9202e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(862, 34)\n"
     ]
    }
   ],
   "source": [
    "# testing our extract_features function:\n",
    "first_row = merged_data.iloc[0]\n",
    "file_path = os.path.join(first_row['path'], first_row['filename'])\n",
    "\n",
    "# Extract features directly from the file path\n",
    "features = extract_features_time_series(file_path, sr=16000, n_mfcc=20, duration=3.0)\n",
    "print(features.shape)  # The shape should be [time_frames, features]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1219d9f3",
   "metadata": {},
   "source": [
    "Old Approach (Feature Aggregation):\n",
    "In the old approach, you were extracting summary statistics (mean, std, skewness, kurtosis) of features like MFCCs, spectral centroid, etc.\n",
    "This method is useful when you want to transform time-series data into a set of static features, which can then be used with traditional machine learning models like Random Forests, SVMs, or Feedforward Neural Networks.\n",
    "However, it loses temporal information. You're summarizing the entire time-series into a handful of values, so models that could learn sequence dependencies (like RNNs, LSTMs, GRUs, or CNNs) wouldn't be able to take full advantage of the temporal structure of the data.\n",
    "\n",
    "#### New Time-Series Approach:\n",
    "Preserving the Time-Series Structure: Instead of reducing the time-series to summary statistics, the core idea is to feed the entire sequence of features (MFCCs, Chroma, etc.) into models that can learn from the temporal relationships between frames.\n",
    "This means RNNs, LSTMs, or even CNN-LSTMs can now model the sequential dependencies in the audio, which may be crucial for accurately predicting emotions.\n",
    "Why Temporal Data Matters: Emotions in speech are often a function of how certain features evolve over time. For instance, a rising pitch or a slower rhythm can indicate different emotional states, and these characteristics can only be captured if you preserve the time dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588c0862",
   "metadata": {},
   "source": [
    "Now that we've validated our extract_features function, we can apply it to the rest of our dataframe.\n",
    "\n",
    "**Notes**: \n",
    "- This cell can take a while to run! About 5 minutes\n",
    "- suppressed UserWarning: Trying to estimate tuning from empty frequency set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "040e09d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\Anaconda\\envs\\learn-env\\lib\\site-packages\\librosa\\core\\pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n",
      "  return pitch_tuning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       id             filename  emotion           path  \\\n",
      "0  c_0001  1001_DFA_ANG_XX.wav    angry  dataset\\Crema   \n",
      "1  c_0002  1001_DFA_DIS_XX.wav  disgust  dataset\\Crema   \n",
      "2  c_0003  1001_DFA_FEA_XX.wav     fear  dataset\\Crema   \n",
      "3  c_0004  1001_DFA_HAP_XX.wav    happy  dataset\\Crema   \n",
      "4  c_0005  1001_DFA_NEU_XX.wav  neutral  dataset\\Crema   \n",
      "\n",
      "                                               mfccs  \\\n",
      "0  [[-589.8760375976562, 47.880348205566406, 34.0...   \n",
      "1  [[-494.1741027832031, 107.23338317871094, 34.1...   \n",
      "2  [[-466.0972900390625, 106.16978454589844, 33.1...   \n",
      "3  [[-465.77593994140625, 85.6107406616211, 47.10...   \n",
      "4  [[-484.9637145996094, 112.21417236328125, 39.0...   \n",
      "\n",
      "                                              chroma  \\\n",
      "0  [[0.8681901097297668, 0.8324087858200073, 0.80...   \n",
      "1  [[0.5067101120948792, 0.8211945295333862, 0.64...   \n",
      "2  [[0.6143918037414551, 1.0, 0.5763747096061707,...   \n",
      "3  [[0.23183658719062805, 0.2823021113872528, 0.4...   \n",
      "4  [[0.10831509530544281, 0.04355860874056816, 0....   \n",
      "\n",
      "                                   spectral_centroid  \\\n",
      "0  [1319.6803872191083, 1173.8460792183148, 1190....   \n",
      "1  [1180.9858712193702, 1197.4237182790616, 1115....   \n",
      "2  [1291.4565400296353, 1240.9293681854763, 1297....   \n",
      "3  [1535.1231379899987, 1312.125645938991, 1364.0...   \n",
      "4  [1125.1491697427234, 1094.2834160756547, 1121....   \n",
      "\n",
      "                                  zero_crossing_rate  \n",
      "0  [0.04833984375, 0.05712890625, 0.0673828125, 0...  \n",
      "1  [0.01953125, 0.03125, 0.0380859375, 0.04052734...  \n",
      "2  [0.0166015625, 0.03173828125, 0.04150390625, 0...  \n",
      "3  [0.02490234375, 0.0341796875, 0.0498046875, 0....  \n",
      "4  [0.015625, 0.02490234375, 0.0341796875, 0.0371...  \n"
     ]
    }
   ],
   "source": [
    "# Initialize empty lists to store extracted features\n",
    "mfcc_series_list = []\n",
    "chroma_series_list = []\n",
    "spectral_centroid_series_list = []\n",
    "zero_crossing_rate_series_list = []\n",
    "\n",
    "# Iterate over each row in the DataFrame and extract features\n",
    "for index, row in merged_data.iterrows():\n",
    "    file_path = os.path.join(row['path'], row['filename'])  # Construct the file path\n",
    "    \n",
    "    # Extract time-series features using the feature extraction function\n",
    "    features = extract_features_time_series(file_path, sr=16000, n_mfcc=20, duration=3.0)\n",
    "    \n",
    "    if features is not None:\n",
    "        # Split the features matrix back into components\n",
    "        mfccs = features[:, :20]  # First 20 columns are MFCCs\n",
    "        chroma = features[:, 20:32]  # Next 12 columns are Chroma features\n",
    "        spectral_centroid = features[:, 32]  # Spectral centroid (1 column)\n",
    "        zero_crossing_rate = features[:, 33]  # Zero-crossing rate (1 column)\n",
    "\n",
    "        # Append the extracted features to the respective lists\n",
    "        mfcc_series_list.append(mfccs)\n",
    "        chroma_series_list.append(chroma)\n",
    "        spectral_centroid_series_list.append(spectral_centroid)\n",
    "        zero_crossing_rate_series_list.append(zero_crossing_rate)\n",
    "    else:\n",
    "        # If feature extraction returns None, raise an error\n",
    "        raise ValueError(f\"Feature extraction failed for file: {file_path}\")\n",
    "\n",
    "# Add the extracted features into the DataFrame as new columns\n",
    "merged_data['mfccs'] = mfcc_series_list\n",
    "merged_data['chroma'] = chroma_series_list\n",
    "merged_data['spectral_centroid'] = spectral_centroid_series_list\n",
    "merged_data['zero_crossing_rate'] = zero_crossing_rate_series_list\n",
    "\n",
    "# Check the updated DataFrame\n",
    "print(merged_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4a6e47",
   "metadata": {},
   "source": [
    "### Cleaning the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37900da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'filename', 'emotion', 'path', 'mfccs', 'chroma',\n",
      "       'spectral_centroid', 'zero_crossing_rate'],\n",
      "      dtype='object')\n",
      "                                               mfccs  \\\n",
      "0  [[-589.8760375976562, 47.880348205566406, 34.0...   \n",
      "1  [[-494.1741027832031, 107.23338317871094, 34.1...   \n",
      "2  [[-466.0972900390625, 106.16978454589844, 33.1...   \n",
      "3  [[-465.77593994140625, 85.6107406616211, 47.10...   \n",
      "4  [[-484.9637145996094, 112.21417236328125, 39.0...   \n",
      "\n",
      "                                   spectral_centroid  \\\n",
      "0  [1319.6803872191083, 1173.8460792183148, 1190....   \n",
      "1  [1180.9858712193702, 1197.4237182790616, 1115....   \n",
      "2  [1291.4565400296353, 1240.9293681854763, 1297....   \n",
      "3  [1535.1231379899987, 1312.125645938991, 1364.0...   \n",
      "4  [1125.1491697427234, 1094.2834160756547, 1121....   \n",
      "\n",
      "                                              chroma  \\\n",
      "0  [[0.8681901097297668, 0.8324087858200073, 0.80...   \n",
      "1  [[0.5067101120948792, 0.8211945295333862, 0.64...   \n",
      "2  [[0.6143918037414551, 1.0, 0.5763747096061707,...   \n",
      "3  [[0.23183658719062805, 0.2823021113872528, 0.4...   \n",
      "4  [[0.10831509530544281, 0.04355860874056816, 0....   \n",
      "\n",
      "                                  zero_crossing_rate           path  \\\n",
      "0  [0.04833984375, 0.05712890625, 0.0673828125, 0...  dataset\\Crema   \n",
      "1  [0.01953125, 0.03125, 0.0380859375, 0.04052734...  dataset\\Crema   \n",
      "2  [0.0166015625, 0.03173828125, 0.04150390625, 0...  dataset\\Crema   \n",
      "3  [0.02490234375, 0.0341796875, 0.0498046875, 0....  dataset\\Crema   \n",
      "4  [0.015625, 0.02490234375, 0.0341796875, 0.0371...  dataset\\Crema   \n",
      "\n",
      "              filename  emotion  \n",
      "0  1001_DFA_ANG_XX.wav    angry  \n",
      "1  1001_DFA_DIS_XX.wav  disgust  \n",
      "2  1001_DFA_FEA_XX.wav     fear  \n",
      "3  1001_DFA_HAP_XX.wav    happy  \n",
      "4  1001_DFA_NEU_XX.wav  neutral  \n"
     ]
    }
   ],
   "source": [
    "# Check the column names to ensure they match your expectations\n",
    "print(merged_data.columns)\n",
    "\n",
    "# Create a new DataFrame for the cleaned features\n",
    "clean_data = pd.DataFrame()\n",
    "\n",
    "# Directly assign the columns if you're certain they are valid (this avoids apply())\n",
    "# You can remove the type check if you're confident in your data.\n",
    "clean_data['mfccs'] = merged_data['mfccs'].apply(\n",
    "    lambda x: x if isinstance(x, (list, np.ndarray)) else np.nan)\n",
    "\n",
    "clean_data['spectral_centroid'] = merged_data['spectral_centroid'].apply(\n",
    "    lambda x: x if isinstance(x, (list, np.ndarray)) else np.nan)\n",
    "\n",
    "clean_data['chroma'] = merged_data['chroma'].apply(\n",
    "    lambda x: x if isinstance(x, (list, np.ndarray)) else np.nan)\n",
    "\n",
    "clean_data['zero_crossing_rate'] = merged_data['zero_crossing_rate'].apply(\n",
    "    lambda x: x if isinstance(x, (list, np.ndarray)) else np.nan)\n",
    "\n",
    "# Copy over the metadata columns directly\n",
    "clean_data['path'] = merged_data['path']\n",
    "clean_data['filename'] = merged_data['filename']\n",
    "clean_data['emotion'] = merged_data['emotion']\n",
    "\n",
    "# Check the cleaned DataFrame\n",
    "print(clean_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5569f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mfccs                 0\n",
      "spectral_centroid     0\n",
      "chroma                0\n",
      "zero_crossing_rate    0\n",
      "path                  0\n",
      "filename              0\n",
      "emotion               0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for any NaN values in the DataFrame\n",
    "print(clean_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4097d274",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# distribution of emotions\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=clean_data, x='emotion', order=clean_data['emotion'].value_counts().index)\n",
    "plt.title('Distribution of Emotions')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# this chart shows some a significant imbalance in the dataset. lets do some resampling\n",
    "# Lets try using the SMOTE technique (Synthetic Minority Over-sampling):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b7084ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique emotions after mapping: ['angry' 'disgust' 'fear' 'happy' 'neutral' 'sad' 'surprise']\n"
     ]
    }
   ],
   "source": [
    "# Convert all emotion labels to lowercase to standardize\n",
    "clean_data['emotion'] = clean_data['emotion'].str.lower()\n",
    "\n",
    "# Map similar emotions to a single label\n",
    "clean_data['emotion'] = clean_data['emotion'].replace({\n",
    "    'surprised': 'surprise',\n",
    "    'sadness': 'sad',\n",
    "    'fearful': 'fear',\n",
    "    'pleasant': 'happy',\n",
    "    'calm': 'neutral',\n",
    "    'surprise': 'neutral'\n",
    "})\n",
    "\n",
    "clean_data = clean_data.loc[clean_data['emotion'] != 'unknown']\n",
    "\n",
    "# Check the unique values after standardization\n",
    "unique_emotions = clean_data['emotion'].unique()\n",
    "print(\"Unique emotions after mapping:\", unique_emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "554431dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mfccs</th>\n",
       "      <th>spectral_centroid</th>\n",
       "      <th>chroma</th>\n",
       "      <th>zero_crossing_rate</th>\n",
       "      <th>path</th>\n",
       "      <th>filename</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[-589.8760375976562, 47.880348205566406, 34.0...</td>\n",
       "      <td>[1319.6803872191083, 1173.8460792183148, 1190....</td>\n",
       "      <td>[[0.8681901097297668, 0.8324087858200073, 0.80...</td>\n",
       "      <td>[0.04833984375, 0.05712890625, 0.0673828125, 0...</td>\n",
       "      <td>dataset\\Crema</td>\n",
       "      <td>1001_DFA_ANG_XX.wav</td>\n",
       "      <td>angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[-494.1741027832031, 107.23338317871094, 34.1...</td>\n",
       "      <td>[1180.9858712193702, 1197.4237182790616, 1115....</td>\n",
       "      <td>[[0.5067101120948792, 0.8211945295333862, 0.64...</td>\n",
       "      <td>[0.01953125, 0.03125, 0.0380859375, 0.04052734...</td>\n",
       "      <td>dataset\\Crema</td>\n",
       "      <td>1001_DFA_DIS_XX.wav</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[-466.0972900390625, 106.16978454589844, 33.1...</td>\n",
       "      <td>[1291.4565400296353, 1240.9293681854763, 1297....</td>\n",
       "      <td>[[0.6143918037414551, 1.0, 0.5763747096061707,...</td>\n",
       "      <td>[0.0166015625, 0.03173828125, 0.04150390625, 0...</td>\n",
       "      <td>dataset\\Crema</td>\n",
       "      <td>1001_DFA_FEA_XX.wav</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[-465.77593994140625, 85.6107406616211, 47.10...</td>\n",
       "      <td>[1535.1231379899987, 1312.125645938991, 1364.0...</td>\n",
       "      <td>[[0.23183658719062805, 0.2823021113872528, 0.4...</td>\n",
       "      <td>[0.02490234375, 0.0341796875, 0.0498046875, 0....</td>\n",
       "      <td>dataset\\Crema</td>\n",
       "      <td>1001_DFA_HAP_XX.wav</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[-484.9637145996094, 112.21417236328125, 39.0...</td>\n",
       "      <td>[1125.1491697427234, 1094.2834160756547, 1121....</td>\n",
       "      <td>[[0.10831509530544281, 0.04355860874056816, 0....</td>\n",
       "      <td>[0.015625, 0.02490234375, 0.0341796875, 0.0371...</td>\n",
       "      <td>dataset\\Crema</td>\n",
       "      <td>1001_DFA_NEU_XX.wav</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               mfccs  \\\n",
       "0  [[-589.8760375976562, 47.880348205566406, 34.0...   \n",
       "1  [[-494.1741027832031, 107.23338317871094, 34.1...   \n",
       "2  [[-466.0972900390625, 106.16978454589844, 33.1...   \n",
       "3  [[-465.77593994140625, 85.6107406616211, 47.10...   \n",
       "4  [[-484.9637145996094, 112.21417236328125, 39.0...   \n",
       "\n",
       "                                   spectral_centroid  \\\n",
       "0  [1319.6803872191083, 1173.8460792183148, 1190....   \n",
       "1  [1180.9858712193702, 1197.4237182790616, 1115....   \n",
       "2  [1291.4565400296353, 1240.9293681854763, 1297....   \n",
       "3  [1535.1231379899987, 1312.125645938991, 1364.0...   \n",
       "4  [1125.1491697427234, 1094.2834160756547, 1121....   \n",
       "\n",
       "                                              chroma  \\\n",
       "0  [[0.8681901097297668, 0.8324087858200073, 0.80...   \n",
       "1  [[0.5067101120948792, 0.8211945295333862, 0.64...   \n",
       "2  [[0.6143918037414551, 1.0, 0.5763747096061707,...   \n",
       "3  [[0.23183658719062805, 0.2823021113872528, 0.4...   \n",
       "4  [[0.10831509530544281, 0.04355860874056816, 0....   \n",
       "\n",
       "                                  zero_crossing_rate           path  \\\n",
       "0  [0.04833984375, 0.05712890625, 0.0673828125, 0...  dataset\\Crema   \n",
       "1  [0.01953125, 0.03125, 0.0380859375, 0.04052734...  dataset\\Crema   \n",
       "2  [0.0166015625, 0.03173828125, 0.04150390625, 0...  dataset\\Crema   \n",
       "3  [0.02490234375, 0.0341796875, 0.0498046875, 0....  dataset\\Crema   \n",
       "4  [0.015625, 0.02490234375, 0.0341796875, 0.0371...  dataset\\Crema   \n",
       "\n",
       "              filename  emotion  \n",
       "0  1001_DFA_ANG_XX.wav    angry  \n",
       "1  1001_DFA_DIS_XX.wav  disgust  \n",
       "2  1001_DFA_FEA_XX.wav     fear  \n",
       "3  1001_DFA_HAP_XX.wav    happy  \n",
       "4  1001_DFA_NEU_XX.wav  neutral  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=clean_data, x='emotion', order=clean_data['emotion'].value_counts().index)\n",
    "plt.title('Distribution of Emotions')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "clean_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4028ea86",
   "metadata": {},
   "source": [
    "## Previous Model Iterations / Evaluations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c60f88f",
   "metadata": {},
   "source": [
    "#### Evaluation v1:\n",
    "\n",
    "Classification Report (grid search)\n",
    "\n",
    "|               | precision | recall | f1-score | support |\n",
    "|---------------|-----------|--------|----------|---------|\n",
    "| accuracy      |           |        |          | 0.46    | 2433    |\n",
    "| macro avg     | 0.43      | 0.43   | 0.43     | 2433    |\n",
    "| weighted avg  | 0.46      | 0.46   | 0.45     | 2433    |\n",
    "\n",
    "#### Evaluation v2:\n",
    "\n",
    "Classification Report (grid search with SMOTE):\n",
    "\n",
    "|               | precision | recall | f1-score | support |\n",
    "|---------------|-----------|--------|----------|---------|\n",
    "| accuracy      |           |        |          | 0.71    | 5962    |\n",
    "| macro avg     | 0.70      | 0.71   | 0.70     | 5962    |\n",
    "| weighted avg  | 0.70      | 0.71   | 0.71     | 5962    |\n",
    "\n",
    "#### Evaluation v3:\n",
    "\n",
    "Classification Report (decision tree w/ augmentation and 1500 samples per category):\n",
    "\n",
    "|               | precision | recall | f1-score | support |\n",
    "|---------------|-----------|--------|----------|---------|\n",
    "| accuracy      |           |        |          | 0.58    | 2700    |\n",
    "| macro avg     | 0.57      | 0.58   | 0.57     | 2700    |\n",
    "| weighted avg  | 0.57      | 0.58   | 0.57     | 2700    |\n",
    "\n",
    "Accuracy of 59%; certainly an imporovement! But nothing substantial. \n",
    "\n",
    "Despite evening the distribution, we are still not seeing the model perform well. This is likely because 7k records\n",
    "is probably not enough to get good predictive capacity over our 7 categories. \n",
    "\n",
    "the f1-scores that indicate weaknesses in our model is: disgust, fear, happy. this tells me that maybe we shouldn't have deleted \n",
    "\n",
    "Let's engineer the categories into pos/neg binary classification and see if that improves our results.\n",
    "\n",
    "#### Evaluation v4:\n",
    "\n",
    "Classification Report (decision tree w/ even distribution, no deleting records):\n",
    "\n",
    "|               | precision | recall | f1-score | support |\n",
    "|---------------|-----------|--------|----------|---------|\n",
    "| accuracy      |           |        |          | 0.56    | 3135    |\n",
    "| macro avg     | 0.57      | 0.59   | 0.57     | 3135    |\n",
    "| weighted avg  | 0.55      | 0.56   | 0.54     | 3135    |\n",
    "\n",
    "Accuracy of 55.34%: While this may seem like a drop, it's important to note that the dataset has been augmented, which can affect the accuracy score. \n",
    "\n",
    "F1-scores: \n",
    "- the f1-scores for underrepresented classes like 'disgust', 'fear', and 'happy' slightly improved compared to previous version.\n",
    "- the f1-score for 'neutral' decreased slightly (0.02), which could be due to the additional augmented samples\n",
    "- the f1-score for the other classes either remained or improved marginally\n",
    "\n",
    "Precision / Recall:\n",
    "- the recall for underrepresented classes has generally improved\n",
    "- however, the precision (ability to not label negative instances as positive) has decreased, which could be due to the introduction of noise in the augmented data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22be619",
   "metadata": {},
   "source": [
    "#### Evaluation v5\n",
    "\n",
    "Classification Report (logistic regression for **binary classification**):  \n",
    "\n",
    "| | precision | recall | f1-score | support |\n",
    "|---------------|-----------|--------|----------|---------|\n",
    "| Negative | 0.65 | 0.74 | 0.69 | 1536 |\n",
    "| Positive | 0.36 | 0.27 | 0.31 | 825 |\n",
    "| accuracy | | | | 0.57 | 2361 |\n",
    "| macro avg | 0.51 | 0.51 | 0.50 | 2361 |\n",
    "| weighted avg | 0.55 | 0.57 | 0.56 | 2361 |\n",
    "\n",
    "As suspected, the 'positive' data suffered due to imbalanced classes. \n",
    "\n",
    "Let's augment some of the audio for the 'positive' class, and bring it closer to the negative class balance.\n",
    "\n",
    "#### Evaluation v6\n",
    "Classification Report (logistic regression for **binary classification with augmented data**): \n",
    "\n",
    "| | precision | recall | f1-score | support |\n",
    "|---------------|-----------|--------|----------|---------|\n",
    "| Negative | 0.65 | 0.74 | 0.69 | 1536 |\n",
    "| Positive | 0.36 | 0.27 | 0.31 | 825 |\n",
    "| accuracy | | | | 0.57 | 2361 |\n",
    "| macro avg | 0.51 | 0.51 | 0.50 | 2361 |\n",
    "| weighted avg | 0.55 | 0.57 | 0.56 | 2361 |\n",
    "\n",
    "Having the same values as v5, I believe where we split the data caused data leakage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143b9066",
   "metadata": {},
   "source": [
    "#### Evaluation v7\n",
    "\n",
    "Classification Report (neural network for multiclassification):\n",
    "\n",
    "| Emotion   | Precision | Recall | F1-Score | Support |\n",
    "|-----------|-----------|--------|----------|---------|\n",
    "| angry     | 0.66      | 0.60   | 0.63     | 405     |\n",
    "| calm      | 0.74      | 0.86   | 0.79     | 291     |\n",
    "| disgust   | 0.52      | 0.30   | 0.38     | 383     |\n",
    "| fear      | 0.71      | 0.22   | 0.34     | 388     |\n",
    "| happy     | 0.39      | 0.40   | 0.39     | 364     |\n",
    "| neutral   | 0.42      | 0.57   | 0.49     | 367     |\n",
    "| pleasant  | 0.75      | 0.94   | 0.84     | 283     |\n",
    "| sad       | 0.49      | 0.59   | 0.53     | 374     |\n",
    "| surprise  | 0.58      | 0.89   | 0.70     | 280     |\n",
    "| **accuracy** |           |        | 0.57     | 3135    |\n",
    "| **macro avg** | 0.59      | 0.60   | 0.57     | 3135    |\n",
    "| **weighted avg** | 0.58      | 0.57   | 0.55     | 3135    |\n",
    "\n",
    "Not much better. Tried better feature extraction for only 2% increase (59%)\n",
    "\n",
    "\n",
    "#### Evaluation v8:\n",
    "\n",
    "Classification Report (Convolutional Neural Network with better features):\n",
    "\n",
    "| Emotion    | Precision | Recall | F1-Score | Support |\n",
    "|------------|-----------|--------|----------|---------|\n",
    "| Angry      | 0.72      | 0.70   | 0.71     | 405     |\n",
    "| Calm       | 0.77      | 0.97   | 0.86     | 291     |\n",
    "| Disgust    | 0.45      | 0.31   | 0.37     | 383     |\n",
    "| Fear       | 0.63      | 0.27   | 0.38     | 388     |\n",
    "| Happy      | 0.46      | 0.43   | 0.44     | 364     |\n",
    "| Neutral    | 0.49      | 0.63   | 0.55     | 367     |\n",
    "| Pleasant   | 0.88      | 0.94   | 0.91     | 283     |\n",
    "| Sad        | 0.51      | 0.63   | 0.57     | 374     |\n",
    "| Surprise   | 0.70      | 0.94   | 0.80     | 280     |\n",
    "| **Accuracy** |           |        | **0.62** | 3135    |\n",
    "| **Macro Avg** | 0.62      | 0.65   | 0.62     | 3135    |\n",
    "| **Weighted Avg** | 0.61      | 0.62   | 0.60     | 3135    |\n",
    "\n",
    "Seeing some progress! This makes sense, since CNN's are better for audio data. Lets try a more advanced CNN Architecture, this time using time series data - rather than pulling generic features to analyze. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e52e24",
   "metadata": {},
   "source": [
    "### Just in Case\n",
    "\n",
    "As you can see, in some of our previous models, we used augmented audio. Though we did not use this function for the final models, I am keeping it in my notebook - just in case!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dcb9b860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# audio augmentation to help with uneven data\n",
    "def augment_audio(data, sr):\n",
    "    # Time Stretching\n",
    "    try:\n",
    "        stretched_data = librosa.effects.time_stretch(data, rate=1.1)\n",
    "    except Exception as e:\n",
    "        stretched_data = data\n",
    "    \n",
    "    # Shifting \n",
    "    shift = np.random.randint(sr)\n",
    "    shifted_data = np.roll(data, shift)\n",
    "    \n",
    "    # Volume adjustment\n",
    "    amplitude_scale = np.random.uniform(low=0.8, high=1.2)\n",
    "    adjusted_volume_data = data * amplitude_scale\n",
    "    \n",
    "    # Randomly choose one of the augmentation methods to apply\n",
    "    augmentation_methods = [stretched_data, shifted_data, adjusted_volume_data]\n",
    "    \n",
    "    # Check for invalid values in augmentation methods\n",
    "    valid_methods = [method for method in augmentation_methods if len(method) > 0 and not np.isnan(method).any() and not np.isinf(method).any()]\n",
    "    \n",
    "    if valid_methods:\n",
    "        augmented_data = random.choice(valid_methods)\n",
    "    else:\n",
    "        augmented_data = data\n",
    "    \n",
    "    return augmented_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25852c3b",
   "metadata": {},
   "source": [
    "## Multiclass Classification Model:\n",
    "\n",
    "We will first attempt to create a classification model with 6 emotion classes:\n",
    "> angry, happy, sad, neutral, disgust, fear\n",
    "\n",
    "Classification Report:\n",
    "\n",
    "| Class      | Precision | Recall | F1-Score | Support |\n",
    "|------------|-----------|--------|----------|---------|\n",
    "| **angry**      | 0.69      | 0.72   | 0.71     | 362     |\n",
    "| **disgust**    | 0.61      | 0.48   | 0.54     | 385     |\n",
    "| **fear**       | 0.68      | 0.41   | 0.51     | 381     |\n",
    "| **happy**      | 0.55      | 0.67   | 0.61     | 448     |\n",
    "| **neutral**    | 0.58      | 0.62   | 0.60     | 340     |\n",
    "| **sad**        | 0.57      | 0.71   | 0.63     | 408     |\n",
    "| **surprise**   | 0.61      | 0.46   | 0.52     | 37      |\n",
    "| **Accuracy**   |           |        | 0.60     | 2361    |\n",
    "| **Macro Avg**  | 0.61      | 0.58   | 0.59     | 2361    |\n",
    "| **Weighted Avg** | 0.61    | 0.60   | 0.60     | 2361    |\n",
    "\n",
    "This model was successfully able to differentiate these emotions with 60% accuracy (much better than random guessing - which would be ~17%). \n",
    "\n",
    "Though, this score isn't incredibly impressive, and the difference between a customer being angry or disgusted wouldn't make a huge difference to the CS agent. Instead, we will create a binary-classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bdb61a",
   "metadata": {},
   "source": [
    "\n",
    "### Encode Labels and Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5f1b3201",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emotion to Encoded Integer Mapping:  {'angry': 0, 'disgust': 1, 'fear': 2, 'happy': 3, 'neutral': 4, 'sad': 5, 'surprise': 6}\n",
      "X_train shape: (9441, 4), y_train shape: (9441, 7)\n",
      "X_test shape: (2361, 4), y_test shape: (2361, 7)\n"
     ]
    }
   ],
   "source": [
    "# Make a copy of the clean_data DataFrame\n",
    "data = clean_data.copy()\n",
    "\n",
    "# Ensure the LabelEncoder is fit on all unique emotion labels\n",
    "le = LabelEncoder()\n",
    "le.fit(data['emotion']) \n",
    "\n",
    "# Encode the emotions for classification\n",
    "data['emotion_encoded'] = le.transform(data['emotion'])  # Store encoded labels in a new column\n",
    "\n",
    "# Create a dictionary to map encoded integers back to the original emotion strings\n",
    "encoded_dict = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "print(\"Emotion to Encoded Integer Mapping: \", encoded_dict)\n",
    "\n",
    "# One-hot encode the labels for neural network output\n",
    "one_hot_encoder = OneHotEncoder(sparse_output=False)  # Ensure sparse_output=False to get a dense array\n",
    "y = one_hot_encoder.fit_transform(data['emotion_encoded'].values.reshape(-1, 1))\n",
    "\n",
    "# Store the labels separately for resampling (if needed)\n",
    "y_labels = data['emotion_encoded']\n",
    "\n",
    "# Drop columns that are not features for model training\n",
    "X = data.drop(['emotion', 'emotion_encoded', 'path', 'filename'], axis=1, errors='ignore')\n",
    "\n",
    "# Split the data into training and test sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test, y_train_labels, y_test_labels = train_test_split(\n",
    "    X, y, y_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Check the shape of the training and test data to confirm everything is correct\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80bc16f",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1d235412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the columns that contain the time-series data\n",
    "columns_to_aggregate = ['mfccs', 'chroma', 'zero_crossing_rate', 'spectral_centroid']\n",
    "\n",
    "# Define a function to pad the time-series data to the same length\n",
    "def pad_time_series(series, target_length):\n",
    "    \"\"\"Pads the time-series to the specified target length with zeros.\"\"\"\n",
    "    return np.pad(series, (0, max(0, target_length - len(series))), 'constant')\n",
    "\n",
    "# Find the maximum length of time-series across all columns (timesteps)\n",
    "def find_max_timesteps(df, columns):\n",
    "    return max(df[columns].applymap(lambda x: len(x)).max())\n",
    "\n",
    "# Pad all time-series columns to have the same number of timesteps (max length)\n",
    "def pad_time_series_df(df, columns, target_length):\n",
    "    for col in columns:\n",
    "        df[col] = df[col].apply(lambda x: pad_time_series(x, target_length))\n",
    "    return df\n",
    "\n",
    "# Find the maximum number of timesteps across all the time-series columns\n",
    "max_timesteps = find_max_timesteps(X_train, columns_to_aggregate)\n",
    "\n",
    "# Pad the time-series data in both training and test datasets\n",
    "X_train_padded = pad_time_series_df(X_train, columns_to_aggregate, max_timesteps)\n",
    "X_test_padded = pad_time_series_df(X_test, columns_to_aggregate, max_timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c7bbe00a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train_3d: (9441, 862, 34)\n",
      "Shape of X_test_3d: (2361, 862, 34)\n"
     ]
    }
   ],
   "source": [
    "def stack_time_series(df, columns):\n",
    "    # Convert each time-series column to a 3D array (samples, timesteps, 1 feature)\n",
    "    stacked_arrays = []\n",
    "    for col in columns:\n",
    "        # Convert each column to a 2D array: (samples, timesteps)\n",
    "        col_array = np.array(df[col].tolist())\n",
    "        \n",
    "        # Check if the array is 2D (i.e., has only (samples, timesteps)), then add a feature dimension\n",
    "        if len(col_array.shape) == 2:\n",
    "            # Add a new axis to make it (samples, timesteps, 1)\n",
    "            col_array = np.expand_dims(col_array, axis=-1)\n",
    "        \n",
    "        # Append the 3D array (samples, timesteps, features)\n",
    "        stacked_arrays.append(col_array)\n",
    "    \n",
    "    # Stack along the feature axis to create a 3D array: (samples, timesteps, features)\n",
    "    return np.concatenate(stacked_arrays, axis=-1)\n",
    "\n",
    "# Convert the padded time-series columns for training and test data\n",
    "X_train_3d = stack_time_series(X_train_padded, columns_to_aggregate)\n",
    "X_test_3d = stack_time_series(X_test_padded, columns_to_aggregate)\n",
    "\n",
    "# Check the shape of the resulting 3D arrays\n",
    "print(\"Shape of X_train_3d:\", X_train_3d.shape)  # Expected shape: (samples, timesteps, features)\n",
    "print(\"Shape of X_test_3d:\", X_test_3d.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47379244",
   "metadata": {},
   "source": [
    "### Define CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0f673d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timesteps: 862, Features: 34\n"
     ]
    }
   ],
   "source": [
    "# Get timesteps and features from the new 3D array\n",
    "timesteps = X_train_3d.shape[1]\n",
    "features = X_train_3d.shape[2]\n",
    "print(f\"Timesteps: {timesteps}, Features: {features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f591e4c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d (Conv1D)             (None, 860, 32)           3296      \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1  (None, 430, 32)           0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 428, 64)           6208      \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPoolin  (None, 214, 64)           0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 214, 128)          8320      \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 27392)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               7012608   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 7)                 455       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7072039 (26.98 MB)\n",
      "Trainable params: 7072039 (26.98 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the CNN model\n",
    "model = Sequential()\n",
    "\n",
    "# First convolutional layer (input_shape = (timesteps, features))\n",
    "model.add(Conv1D(32, kernel_size=3, activation='relu', input_shape=(timesteps, features)))\n",
    "model.add(MaxPooling1D(pool_size=2))  # Max pooling to reduce dimensionality\n",
    "\n",
    "# Second convolutional layer\n",
    "model.add(Conv1D(64, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "# Third convolutional layer\n",
    "model.add(Conv1D(128, kernel_size=1, activation='relu'))\n",
    "\n",
    "# Flatten the output to feed into Dense layers\n",
    "model.add(Flatten())\n",
    "\n",
    "# Fully connected layers\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "\n",
    "# Output layer (number of emotions, use softmax for multiclass classification)\n",
    "model.add(Dense(y_train.shape[1], activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Summary of the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7a60796e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "296/296 [==============================] - 26s 83ms/step - loss: 3.9676 - accuracy: 0.2920 - val_loss: 1.4686 - val_accuracy: 0.4155\n",
      "Epoch 2/20\n",
      "296/296 [==============================] - 24s 82ms/step - loss: 1.4682 - accuracy: 0.4093 - val_loss: 1.4002 - val_accuracy: 0.4337\n",
      "Epoch 3/20\n",
      "296/296 [==============================] - 27s 90ms/step - loss: 1.3590 - accuracy: 0.4430 - val_loss: 1.2851 - val_accuracy: 0.4977\n",
      "Epoch 4/20\n",
      "296/296 [==============================] - 25s 85ms/step - loss: 1.2855 - accuracy: 0.4815 - val_loss: 1.1777 - val_accuracy: 0.5303\n",
      "Epoch 5/20\n",
      "296/296 [==============================] - 24s 80ms/step - loss: 1.2421 - accuracy: 0.4967 - val_loss: 1.1718 - val_accuracy: 0.5210\n",
      "Epoch 6/20\n",
      "296/296 [==============================] - 23s 79ms/step - loss: 1.1940 - accuracy: 0.5205 - val_loss: 1.1206 - val_accuracy: 0.5523\n",
      "Epoch 7/20\n",
      "296/296 [==============================] - 24s 80ms/step - loss: 1.1708 - accuracy: 0.5331 - val_loss: 1.1420 - val_accuracy: 0.5502\n",
      "Epoch 8/20\n",
      "296/296 [==============================] - 23s 79ms/step - loss: 1.1295 - accuracy: 0.5479 - val_loss: 1.0988 - val_accuracy: 0.5684\n",
      "Epoch 9/20\n",
      "296/296 [==============================] - 24s 80ms/step - loss: 1.1189 - accuracy: 0.5612 - val_loss: 1.0819 - val_accuracy: 0.5726\n",
      "Epoch 10/20\n",
      "296/296 [==============================] - 24s 80ms/step - loss: 1.0739 - accuracy: 0.5773 - val_loss: 1.0810 - val_accuracy: 0.5684\n",
      "Epoch 11/20\n",
      "296/296 [==============================] - 24s 79ms/step - loss: 1.1116 - accuracy: 0.5610 - val_loss: 1.0789 - val_accuracy: 0.5722\n",
      "Epoch 12/20\n",
      "296/296 [==============================] - 23s 79ms/step - loss: 1.0540 - accuracy: 0.5863 - val_loss: 1.0801 - val_accuracy: 0.5735\n",
      "Epoch 13/20\n",
      "296/296 [==============================] - 25s 86ms/step - loss: 1.0195 - accuracy: 0.5990 - val_loss: 1.0608 - val_accuracy: 0.5828\n",
      "Epoch 14/20\n",
      "296/296 [==============================] - 25s 84ms/step - loss: 0.9953 - accuracy: 0.6120 - val_loss: 1.0612 - val_accuracy: 0.5879\n",
      "Epoch 15/20\n",
      "296/296 [==============================] - 25s 84ms/step - loss: 0.9719 - accuracy: 0.6187 - val_loss: 1.0211 - val_accuracy: 0.6023\n",
      "Epoch 16/20\n",
      "296/296 [==============================] - 27s 91ms/step - loss: 0.9374 - accuracy: 0.6373 - val_loss: 1.0297 - val_accuracy: 0.6023\n",
      "Epoch 17/20\n",
      "296/296 [==============================] - 25s 85ms/step - loss: 0.9119 - accuracy: 0.6421 - val_loss: 1.0320 - val_accuracy: 0.6065\n",
      "Epoch 18/20\n",
      "296/296 [==============================] - 26s 87ms/step - loss: 0.8926 - accuracy: 0.6537 - val_loss: 1.0523 - val_accuracy: 0.5862\n"
     ]
    }
   ],
   "source": [
    "# Define EarlyStopping callback\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Train the model with EarlyStopping\n",
    "history = model.fit(\n",
    "    X_train_3d, \n",
    "    y_train, \n",
    "    epochs=20, \n",
    "    batch_size=32, \n",
    "    validation_data=(X_test_3d, y_test), \n",
    "    callbacks=[early_stopping] \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9eab02",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9fb59868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.0211, Test Accuracy: 0.6023\n",
      "74/74 [==============================] - 1s 9ms/step\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       angry       0.69      0.72      0.71       362\n",
      "     disgust       0.61      0.48      0.54       385\n",
      "        fear       0.68      0.41      0.51       381\n",
      "       happy       0.55      0.67      0.61       448\n",
      "     neutral       0.58      0.62      0.60       340\n",
      "         sad       0.57      0.71      0.63       408\n",
      "    surprise       0.61      0.46      0.52        37\n",
      "\n",
      "    accuracy                           0.60      2361\n",
      "   macro avg       0.61      0.58      0.59      2361\n",
      "weighted avg       0.61      0.60      0.60      2361\n",
      "\n",
      "Confusion Matrix:\n",
      " [[262  23   5  56  10   5   1]\n",
      " [ 32 185  16  50  33  68   1]\n",
      " [ 28  18 157  76  27  72   3]\n",
      " [ 46  19  34 302  28  14   5]\n",
      " [  3  25   5  40 211  56   0]\n",
      " [  5  32  15  19  48 288   1]\n",
      " [  3   3   0   6   6   2  17]]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test_3d, y_test, verbose=0)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_proba = model.predict(X_test_3d)  # Predicted probabilities\n",
    "y_pred = np.argmax(y_pred_proba, axis=1)  # Get the predicted class labels\n",
    "y_test_classes = np.argmax(y_test, axis=1)  # Get the true class labels\n",
    "\n",
    "# Print evaluation metrics using the zero_division parameter to handle undefined metrics\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test_classes, y_pred, target_names=le.classes_, zero_division=0))\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test_classes, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581ad485",
   "metadata": {},
   "source": [
    "## Binary Classification\n",
    "\n",
    "Our final binary classification model for detecting emotion in voice meets our objective of an average of 80% accuracy overall. Our two classes were:\n",
    "\n",
    "> 0: **Negative** or UPSET\n",
    "\n",
    "\n",
    "\n",
    "> 1: **Positive** or NOT UPSET\n",
    "\n",
    "With the f1-score of 86% for detecting negative emotions, and 68% for detecting positive / neutral emotions: our model is definitely capable of flagging customers when they are upset or not on the phone! This discrepency is likely due to the imbalance of positive/negative values in our dataset. I chose not to augment positive samples for this model, as it seemed to add noise and confusion to the model. \n",
    "\n",
    "*Note: model performance fluctuates on each run.*\n",
    "\n",
    "Classification Report:\n",
    "\n",
    "| Class      | Precision | Recall | F1-Score | Support |\n",
    "|------------|-----------|--------|----------|---------|\n",
    "| **Negative** | 0.80      | 0.94   | 0.86     | 1585    |\n",
    "| **Positive** | 0.81      | 0.51   | 0.63     | 776     |\n",
    "| **Accuracy** |           |        | 0.80     | 2361    |\n",
    "| **Macro Avg** | 0.80      | 0.73   | 0.75     | 2361    |\n",
    "| **Weighted Avg** | 0.80      | 0.80   | 0.79     | 2361    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f39341",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "29fa0604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y_train_binary_encoded: (9441, 2)\n",
      "Shape of y_test_binary_encoded: (2361, 2)\n"
     ]
    }
   ],
   "source": [
    "# Define positive and negative emotions\n",
    "positive_emotions = ['happy', 'neutral', 'surprise']\n",
    "negative_emotions = ['sad', 'angry', 'disgust', 'fear']\n",
    "\n",
    "# Decode integer labels back to original string labels\n",
    "y_train_labels_decoded = le.inverse_transform(y_train_labels)  # Decode integer labels to string labels\n",
    "y_test_labels_decoded = le.inverse_transform(y_test_labels)    # Decode integer labels to string labels\n",
    "\n",
    "# Relabel the dataset for binary classification\n",
    "def relabel_emotions(y, emotion_labels, positive_emotions, negative_emotions):\n",
    "    \"\"\"Convert multiclass emotion labels to binary classification (positive/negative).\"\"\"\n",
    "    binary_labels = []\n",
    "    for label in emotion_labels:\n",
    "        if label in positive_emotions:\n",
    "            binary_labels.append(1)  # 1 for positive emotions\n",
    "        elif label in negative_emotions:\n",
    "            binary_labels.append(0)  # 0 for negative emotions\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown emotion label: {label}\")\n",
    "    return np.array(binary_labels)\n",
    "\n",
    "# Use the decoded string labels for relabeling\n",
    "y_train_binary = relabel_emotions(y_train_labels_decoded, y_train_labels_decoded, positive_emotions, negative_emotions)\n",
    "y_test_binary = relabel_emotions(y_test_labels_decoded, y_test_labels_decoded, positive_emotions, negative_emotions)\n",
    "\n",
    "# Convert the binary labels into a format suitable for model training\n",
    "y_train_binary_encoded = np.eye(2)[y_train_binary]  # Convert 0/1 labels to one-hot encoding\n",
    "y_test_binary_encoded = np.eye(2)[y_test_binary]    # Convert 0/1 labels to one-hot encoding\n",
    "\n",
    "# Check the shape of the binary labels\n",
    "print(f\"Shape of y_train_binary_encoded: {y_train_binary_encoded.shape}\")\n",
    "print(f\"Shape of y_test_binary_encoded: {y_test_binary_encoded.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd562c14",
   "metadata": {},
   "source": [
    "### CNN Model for Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1117f722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_3 (Conv1D)           (None, 860, 32)           3296      \n",
      "                                                                 \n",
      " max_pooling1d_2 (MaxPoolin  (None, 430, 32)           0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " conv1d_4 (Conv1D)           (None, 428, 64)           6208      \n",
      "                                                                 \n",
      " max_pooling1d_3 (MaxPoolin  (None, 214, 64)           0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " conv1d_5 (Conv1D)           (None, 214, 128)          8320      \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 27392)             0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               7012608   \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7071714 (26.98 MB)\n",
      "Trainable params: 7071714 (26.98 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the CNN model for binary classification\n",
    "model = Sequential()\n",
    "\n",
    "# First convolutional layer (input_shape = (timesteps, features))\n",
    "model.add(Conv1D(32, kernel_size=3, activation='relu', input_shape=(X_train_3d.shape[1], X_train_3d.shape[2])))\n",
    "model.add(MaxPooling1D(pool_size=2))  # Max pooling to reduce dimensionality\n",
    "\n",
    "# Second convolutional layer\n",
    "model.add(Conv1D(64, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "# Third convolutional layer\n",
    "model.add(Conv1D(128, kernel_size=1, activation='relu'))\n",
    "\n",
    "# Flatten the output to feed into Dense layers\n",
    "model.add(Flatten())\n",
    "\n",
    "# Fully connected layers\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "\n",
    "# Output layer for binary classification (2 classes: positive/negative)\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Summary of the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f760b66",
   "metadata": {},
   "source": [
    "### Define and Train the CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f35e71e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "296/296 [==============================] - 25s 81ms/step - loss: 2.0630 - accuracy: 0.6382 - val_loss: 0.5739 - val_accuracy: 0.7082\n",
      "Epoch 2/20\n",
      "296/296 [==============================] - 24s 80ms/step - loss: 0.5937 - accuracy: 0.6816 - val_loss: 0.5666 - val_accuracy: 0.6963\n",
      "Epoch 3/20\n",
      "296/296 [==============================] - 24s 80ms/step - loss: 0.5859 - accuracy: 0.6858 - val_loss: 0.5558 - val_accuracy: 0.7128\n",
      "Epoch 4/20\n",
      "296/296 [==============================] - 24s 81ms/step - loss: 0.5763 - accuracy: 0.6958 - val_loss: 0.5339 - val_accuracy: 0.7217\n",
      "Epoch 5/20\n",
      "296/296 [==============================] - 24s 79ms/step - loss: 0.5600 - accuracy: 0.7035 - val_loss: 0.5789 - val_accuracy: 0.7052\n",
      "Epoch 6/20\n",
      "296/296 [==============================] - 24s 80ms/step - loss: 0.5472 - accuracy: 0.7104 - val_loss: 0.5177 - val_accuracy: 0.7344\n",
      "Epoch 7/20\n",
      "296/296 [==============================] - 24s 80ms/step - loss: 0.5219 - accuracy: 0.7251 - val_loss: 0.5075 - val_accuracy: 0.7323\n",
      "Epoch 8/20\n",
      "296/296 [==============================] - 24s 80ms/step - loss: 0.5097 - accuracy: 0.7288 - val_loss: 0.4815 - val_accuracy: 0.7565\n",
      "Epoch 9/20\n",
      "296/296 [==============================] - 23s 79ms/step - loss: 0.5146 - accuracy: 0.7309 - val_loss: 0.5025 - val_accuracy: 0.7382\n",
      "Epoch 10/20\n",
      "296/296 [==============================] - 24s 80ms/step - loss: 0.4907 - accuracy: 0.7450 - val_loss: 0.4979 - val_accuracy: 0.7526\n",
      "Epoch 11/20\n",
      "296/296 [==============================] - 24s 80ms/step - loss: 0.4835 - accuracy: 0.7472 - val_loss: 0.4723 - val_accuracy: 0.7548\n",
      "Epoch 12/20\n",
      "296/296 [==============================] - 24s 80ms/step - loss: 0.4698 - accuracy: 0.7523 - val_loss: 0.4795 - val_accuracy: 0.7620\n",
      "Epoch 13/20\n",
      "296/296 [==============================] - 23s 79ms/step - loss: 0.4631 - accuracy: 0.7619 - val_loss: 0.4731 - val_accuracy: 0.7620\n",
      "Epoch 14/20\n",
      "296/296 [==============================] - 25s 86ms/step - loss: 0.4567 - accuracy: 0.7637 - val_loss: 0.4725 - val_accuracy: 0.7615\n",
      "Test Loss: 0.4723, Test Accuracy: 0.7548\n"
     ]
    }
   ],
   "source": [
    "# Define EarlyStopping callback\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train_3d, \n",
    "    y_train_binary_encoded, \n",
    "    epochs=20, \n",
    "    batch_size=32, \n",
    "    validation_data=(X_test_3d, y_test_binary_encoded),\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test_3d, y_test_binary_encoded, verbose=0)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0541a711",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8f16f781",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74/74 [==============================] - 1s 10ms/step\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.75      0.93      0.83      1536\n",
      "    Positive       0.77      0.42      0.55       825\n",
      "\n",
      "    accuracy                           0.75      2361\n",
      "   macro avg       0.76      0.68      0.69      2361\n",
      "weighted avg       0.76      0.75      0.73      2361\n",
      "\n",
      "Confusion Matrix:\n",
      " [[1433  103]\n",
      " [ 476  349]]\n"
     ]
    }
   ],
   "source": [
    "# Predict on the test set\n",
    "y_pred_proba = model.predict(X_test_3d)  # Predicted probabilities\n",
    "y_pred = np.argmax(y_pred_proba, axis=1)  # Get the predicted class labels (0 or 1)\n",
    "y_test_classes = np.argmax(y_test_binary_encoded, axis=1)  # Get the true class labels (0 or 1)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test_classes, y_pred, target_names=['Negative', 'Positive'], zero_division=0))\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test_classes, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f1a23a",
   "metadata": {},
   "source": [
    "---\n",
    "## Evaluation of Final Model: Binary Classification\n",
    "\n",
    "#### Justification\n",
    "\n",
    "For this emotion detection model, **precision**, **recall**, and **f1-score** are crucial metrics, as both false-positives (incorrectly flagging calm customers) and false-negatives (missing upset customers) can impact customer satisfaction.\n",
    "\n",
    "- **Precision**:\n",
    "  - **Negative (calm)**: 80% of predicted calm customers were actually calm.\n",
    "  - **Positive (upset)**: 81% of predicted upset customers were actually upset.\n",
    "  \n",
    "- **Recall**:\n",
    "  - **Negative (calm)**: 94% of calm customers were correctly identified.\n",
    "  - **Positive (upset)**: 51% of upset customers were correctly identified.\n",
    "  \n",
    "- **F1-Score**:\n",
    "  - **Negative (calm)**: 86%\n",
    "  - **Positive (upset)**: 63%\n",
    "\n",
    "This model is effective for:\n",
    "- **Operational Efficiency**: The model can alert agents when a customer is upset, allowing them to adjust their approach in real-time, improving customer experience.\n",
    "- **Escalation Control**: By tracking how long a customer remains upset, the model can help escalate calls to specialized teams, ensuring that frustrated customers receive the attention they need before issues escalate further.\n",
    "- **Agent Training and Feedback**: The model provides valuable performance metrics, which can be used to give specific feedback to agents on how they handle both positive and negative calls, improving overall service quality."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
